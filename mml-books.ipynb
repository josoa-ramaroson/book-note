{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0d135a-f09d-49ee-952f-fa24f2a44570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.text_cell_render {\n",
       "    line-height: 1.5 !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "div.text_cell_render {\n",
    "    line-height: 1.5 !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf8022-8984-4039-8f44-2b47380c29d2",
   "metadata": {},
   "source": [
    "# Chapter 5 : Vector Calculus\n",
    "(see copy book for the missing sections)\n",
    "## 5.6 Backpropagation and Automatic Differentiation\n",
    "(see notebook for missing subsections)\n",
    "### 5.6.2 Automatic Differentiation\n",
    "- Automatic differentiation applies a series of elementary arithmetic operations, e.g., addition and multiplication and elementary functions, e.g., sin, cos, exp, log.\n",
    "- by applying the chain rule to these operations, the gradient of quite complicated functions can be computed automatically.\n",
    "- from dataflow between x and y, and by applying intermediate variables a,b, we can obtain this:\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\frac{dy}{dx} = \\frac{dy}{db} \\cdot \\frac{db}{da} \\cdot \\frac{da}{dx} \\tag{5.119}\n",
    "\\end{align}\n",
    " $$\n",
    "- Intuitively, the forward and reverse mode differ in the order of multiplication. Due to the associativity of matrix multiplication, we can choose between :\n",
    "$$ \n",
    "\\begin{align}\n",
    "  \\frac{dy}{dx} = \\left( \\frac{dy}{db} \\cdot \\frac{db}{da} \\right) \\cdot \\frac{da}{dx} \\tag{5.120} \\\\\n",
    "  \\frac{dy}{dx} = \\frac{dy}{db} \\cdot \\left( \\frac{db}{da} \\cdot \\frac{da}{dx} \\right) \\tag{5.121}\n",
    "\\end{align}\n",
    "$$ \n",
    "- reverse mode (5.120) :  gradients are propagated backward through the graph, i.e., reverse to the data flow. (backpropagation) the reverse mode is computationally significantly cheaper than the forward mode in Neural Networks.\n",
    "- forward mode (5.121): where the gradients flow with the data from left to right through the graph.\n",
    "- exemple : see P162 which explains the concept of intermediate variables Computation graph\n",
    "with inputs x, function values f ,  and intermediate variables a, b, c, d, e.The set of equations that include intermediate variables can be thought of as a computation graph. In this examples, it shows how we can obtain $ \\frac{\\partial f}{\\partial x} $ . we observe that the computation required for calculating the derivative is of similar complexity as the computation of the function itself.\n",
    "- **Formalization** : Let $x_1 , . . . , x_d$ be the input variables to the function, $x_d+1 , . . . , x_D−1$ be the intermediate variables, and $x_D$ the output variable. Then the computation graph can be expressed as follows:\n",
    "    $$  \\text{For  }  i = d+1,...,D:   x_i = g_i\\left(x_{\\text{Pa}(x_i)}\\right) $$\n",
    "  Where the $ g_i\\left(\\cdot\\right) $ are elementary functions and $  x_{\\text{Pa}(x_i)} $ are the parent nodes of the variable xi in the graph.\n",
    "  For other variables $ x_i $, we apply the chain rule :\n",
    "$$\n",
    "   \\frac{\\partial f} {\\partial x_i}\n",
    "    = \\sum_{x_j : x_i \\in Pa(x_j)} \n",
    "        \\frac{\\partial f}{\\partial x_j} \\,\n",
    "        \\frac{\\partial x_j}{\\partial x_i}\n",
    "    = \\sum_{x_j : x_i \\in Pa(x_j)}\n",
    "        \\frac{\\partial f}{\\partial g_j} \\,\n",
    "        \\frac{\\partial g_j}{\\partial x}\n",
    "        \\tag{5.145}\n",
    "  $$\n",
    "Where $ \\text{Pa} \\left( x_j \\right) $ is the set of parent nodes of $x_j$ in the computation graph.  (5.145) is the backpropagation of the gradient through the computation graph.\n",
    "- For neural network training, we backpropagate the error of the prediction with respect to the label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fc2a54-96f8-4001-aec4-6e852c923f04",
   "metadata": {},
   "source": [
    "## 5.7 Higher-Order Derivatives\n",
    "- Sometimes, we are interested in derivatives of higher order, e.g., when we want to use Newton’s Method for optimization, which requires second-order derivatives (Nocedal and Wright, 2006).\n",
    "- Consider a function $f : \\mathbb{R}^2 \\to \\mathbb{R}$ of two variables $x, y $. We use the following notation for higher-order partial derivatives (and for gradients):\n",
    "  - $\\frac{\\partial^2 f}{\\partial^2 x} $ is the second partial derivative of f with respect to x.\n",
    "  - $\\frac{\\partial^n f}{\\partial^n x} $ is the nth partial derivative of f with respect to x.\n",
    "  - $\\frac{\\partial^2 f}{\\partial y \\cdot \\partial x} = \\frac{\\partial}{\\partial y} \\left( \\frac{\\partial f}{\\partial x} \\right) $  is the partial derivative obtained by first partial differentiating with respect to x and then with respect to y.\n",
    "  - $\\frac{\\partial^2 f}{\\partial x \\cdot \\partial y}$ s the partial derivative obtained by first partial differentiating by y and then x.\n",
    "- *Hessian* : the collection of all second-order partial derivatives\n",
    "- If f (x, y) is a twice (continuously) differentiable function, then  $\\frac{\\partial^2 f}{\\partial y \\cdot \\partial x} = \\frac{\\partial^2 f}{\\partial x \\cdot \\partial y}$; i.e, the order of differentiation does not matter.\n",
    "- We obtain then the *Hessian matrix* :\n",
    "$$\n",
    "      \\textbf{H} =\n",
    "\\begin{bmatrix}\n",
    "   \\frac{\\partial^2 f}{\\partial x^2} &  \\frac{\\partial^2 f}{\\partial x  \\partial y} \\\\\n",
    "  \\frac{\\partial^2 f}{\\partial x  \\partial y} & \\frac{\\partial^2 f}{ \\partial y^2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- This matrix is symmetric. noted $ \\nabla_{x,y}^2 f(x,y)$, which is generaly an n x n matrix for $x \\in \\mathbb{R}^n$\n",
    "- The Hessian measures\n",
    "the curvature of the function locally around (x, y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d467d019-d3b3-4389-843a-d58dd825ab8e",
   "metadata": {},
   "source": [
    "## 5.8 Linearization and Multivariate Taylor Serie\n",
    "- linear approximation of f around $x_0$:\n",
    "  $$\n",
    "  f(x) \\approx f(x_0) + (\\nabla_x f) (x_0) ( x - x_0) \\tag{5.148}\n",
    "  $$\n",
    "   $(\\nabla_x f) (x_0) $ : the gradient of f with respect to x, evaluated at $x_0$ (an input)\n",
    "- The original function is approximated by a straight line. This approximation is locally accurate, but the farther we move away from $x_0$ the worse the approximation gets.\n",
    "- a special case of a multivariate Taylor series expansion of f at $x_0$, where we consider only the first two terms\n",
    "- **Definition of Multivariate Taylor Series** : \n",
    "   - Consider the function : $$\n",
    "  f : \\mathbb{R}^D \\to \\mathbb(R), \n",
    "  \\quad x \\mapsto f(x), \\quad x \\in \\mathbb{R}^D\n",
    "  $$ => is smoot at $x_0$\n",
    "   - When we define the difference vector $\\delta := x - x_0$, the *Multivariate Tailor series* of $f$ at ($x_0$) is defined as\n",
    "     $$\n",
    "     f(x) = \\sum_{k=0}^{\\infty} \\frac{D_{x}^k f(x_0) } {k!} \\, \\delta^k\n",
    "     $$\n",
    "     Where $D_{x}^k f(x_0)$ is the k -th (total) derivative of f with respect to x, evaluated at $x_0$ .\n",
    "- **Taylor Polynomial** :  The Taylor polynomial of degree n of f at x0 contains the first n + 1 components of the series in (5.151) and is defined as :\n",
    "  $$\n",
    "  T_n(x) = \\sum_{k=0}^n \\frac{D_{x}^k f(x_0)} {k!}\\, \\delta^k\n",
    "  $$\n",
    "  $\\delta^k$ is not defined for vectors $x \\in \\mathbb(R)^D, D > 1 \\text{ and } k>1$\n",
    "  Both $D_{x}^k f(x_0)$ and $\\delta^k$ are $k$-th order tensors, i.e, $k$-dimensional arrays.\n",
    "  The $k$th-order tensor $\\delta^k \\in \\mathbb{R}^{D x D x ...xD}$ is obtained as a $k$-fold outer product, denoted \\otimes, of the vector $\\delta \\in \\mathbb{R}^D$. For example $$\n",
    "\\delta^2 := \\delta \\otimes \\delta = \\delta \\, \\delta^T, \\quad\n",
    "\\delta^2 \\!\\left[ i, j \\right] = \\delta_i \\, \\delta_j\n",
    "- Exemple: Taylor Series Expansion of a Function with Two Vari-\n",
    "ables (P 167)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb227e9-d8b0-46e6-a439-7343c48ba0f3",
   "metadata": {},
   "source": [
    "## 5.9 Further Reading (p170)\n",
    "\n",
    "- **Matrix differentials**: Matrix Differential Calculus with Applications in Statistics and Econometrics, Magnus, Jan R., and Neudecker, Heinz. 2007.\n",
    "- **Automatic differentiation**: Elliott, Conal. 2009. Beautiful Differentiation. In: International Conference on Functional Programming. Griewank, Andreas, and Walther, Andrea. 2008. Evaluating Derivatives, Principles and Techniques of Algorithmic Differentiation. SIAM.\n",
    "- **Extended Kalman filter**: Maybeck, Peter S. 1979. Stochastic Models, Estimation, and Control. Academic Press.\n",
    "- **Other deterministic ways** to approximate the integral. unscented transform : Julier, Simon J., and Uhlmann, Jeffrey K. 1997. A New Extension of the Kalman Filter to  nonlinear Systems. In: Proceedings of AeroSense Symposium on Aerospace/Defense Sensing, Simulation and Controls.\n",
    "or the **Laplace approximation** of Murphy, Kevin P. 2012. Machine Learning: A Probabilistic Perspective. MIT Press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f8066-1e21-495f-a9c3-3da59b9a56ac",
   "metadata": {},
   "source": [
    "# Chapter 6: Probability and Distributions\n",
    "- study of uncertainty\n",
    "- use this probability to measure the chance of something occurring in an experiment\n",
    "- *random variable* : Quantifying uncertainty. a function that maps outcomes of random experiments to a set of properties that we are interested in.\n",
    "- *probability distribution* : a function that measures the probability that a particular outcome (or set of outcomes) will occur. used in probabilistic modeling (Section 8.4), graphical models (Section 8.5), and model selection (Section 8.6).\n",
    "- *probability space* :the sample space, the events, and the probability of an event "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e975cd-f374-4f94-bf1a-bba86cbc23a8",
   "metadata": {},
   "source": [
    "## 6.1 Construction of a Probability Space (p172)\n",
    "The theory of probability aims at defining a mathematical structure to describe random outcomes of experiments. (see Jaynes, Edwin T. 2003. Probability Theory: The Logic of Science. Cambridge University Press.)\n",
    "### 6.1.1 Philosophical Issues\n",
    "- For plausible reasoning it is necessary to extend the discrete true and false values of truth\n",
    "to continuous plausibilities\n",
    "- probability theory can be considered a generalization of Boolean logic.\n",
    "- In ML, it is used to formalize the design of automated reasoning systems. (Pearl, Judea. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann.)\n",
    "- plausibility by E. T. Jaynes (1922 - 1998) :\n",
    "      1. The degrees of plausibility are represented by real numbers.\n",
    "      2. These numbers must be based on the rules of common sense.\n",
    "      3. . The resulting reasoning must be consistent, with the three following meanings of the word “consistent”: consistency or non-contradiction, honesty, reproducibility.\n",
    "- **Cox–Jaynes theorem** (p174) : universal mathematical rules that apply to plausibility p\n",
    "- **Remark** : two interpretation of the probability in ML: the Bayesian (*“subjective probability” or “degree of belief”*) and frequentist interpretations (see Bishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.)\n",
    "- in ML, consider wether we are trying to model something categorical (a discrete random variable) or something continuous (a continuous random variable).\n",
    "\n",
    "### 6.1.2 Probability and Random Variables\n",
    "- three idea to not to be confused\n",
    "  - **probability space**: allows us to quantify the idea of a probability\n",
    "  - **random variables** :transfers the probability to a more convenient (often numerical) space.\n",
    "  - **distribution or law associated with a random variable**\n",
    "- from *Grinstead, Charles M., and Snell, J. Laurie. 1997. Introduction to Probability. American Mathematical Society.*, modern probability is based on:\n",
    "    - *The sample space $\\Omega$* (\"state space\" “sample description space”, “possibility space,” and “event space”) : he set of all possible outcomes of the experiment, usually denoted by $\\Omega$.\n",
    "    - *The event space $\\mathcal{A}$* (collection of subsets of $\\Omega$ ) :  the space of potential results of the experiment. A subset $A$ of the sample space $\\Omega$ is in the event space $\\mathcal{A}$ if at the end of the experiment we can observe whether a particular outcome $\\omega \\in \\Omega$ is in $A$.\n",
    "    - *The probabilty $P$* ( $P(A)$ ) : With each event $A \\in \\mathcal{A}$, we associate a number $P(A)$ that measures the probability or degree of belief that the event will occur. in $\\left[0, 1\\right]$ and $P(\\Omega) = 1$ \n",
    "\n",
    "- The *probability space* $(\\Omega, \\mathcal{A}, P)$ models a real-world process or phenomenon. (referred to as an experiment) with random outcomes. In ML, instead we refer to it as *probabilities on quantities of interest* denoted by $\\mathcal{T}$.\n",
    "- $\\mathcal{T}$ : *Taget space*, **element of $\\mathcal{T}$** :states\n",
    "- ***Random Variable*** : It is a function $X: \\Omega \\to \\mathcal{T}$ that takes an element of $\\Omega$ (an outcome) and returns a particular quantity of interest $x$, a value in $\\mathcal{T}$. For any subset $S \\subseteq \\mathcal{T}$ , we associate $P_{X}(S) \\in \\left[0, 1\\right]$ (the probability) to a particular event occurring corresponding to the random variable $X$ .\n",
    "- Exemple (P176) : Consider a statistical experiment where we model a funfair game consisting of drawing two coins from a bag (with replacement) . (see the rest on the book)\n",
    "- *the probability of the output of X $\\neq$ the probability of the samples in $\\Omega$*\n",
    "- $X^{-1}(S)$: Pre-image of $S$  by $X$: The set of elements of $\\Omega$ that map to $S$ under $X$: $\\{\\omega \\subset \\Omega: X(\\omega) \\subset S\\}$\n",
    "- the random variable $X$ is to associate it with the probability of the pre-image of $S$:\n",
    "  $$\n",
    "    P_{X}(S) = P(X \\subset S) = P(X^{-1}(S)) = P (\\{\\omega \\subset \\Omega: X(\\omega) \\subset S\\}) \\tag{6.8}\n",
    "  $$\n",
    "- *Law Distribution* of random variable $X$: the function $P_X$ or equivalently $P \\circ X^{−1}$\n",
    "- **Remark** : The target space, that is, the range $\\mathcal{T}$ of the random variable $X$ , is used to indicate the kind of probability space, i.e., a $\\mathcal{T}$ random variable. When $\\mathcal{T}$ is finite or countably infinite, this is called a discrete random variable (Section 6.2.1). For continuous random variables (Section 6.2.2), we only consider $\\mathcal{T} = \\mathbb{R}$ or $\\mathcal{T} = \\mathbb{R}^D$.\n",
    "\n",
    "### 6.1.3 Statistics\n",
    "- Using probability, we can consider a model of some process, where the underlying uncertainty is captured by random variables, and we use the rules of probability to derive what happens.\n",
    "- In statistics, we observe that something has happened and try to figure out the underlying process that explains the observations.\n",
    "- Thus ML is very close to statistics but We can use the rules of probability to obtain a “best-fitting” model for some data.\n",
    "- in ML, we are intereste in generalization error (perforamance analysis). This analysis of future performance relies on probability and statistics (see *Concentration Inequalities: A Nonasymptotic Theory of Independence. Oxford University Press* or *Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press.*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcfefe8-c4c3-424d-bd92-59b1e8653c68",
   "metadata": {},
   "source": [
    "## 6.2 Discrete and Continuous Probabilities\n",
    "- focus our attention on ways to describe the probability of an event\n",
    "- Depending on whether the target space is discrete or continuous, the natural way to refer to distributions is different\n",
    "- *probability mass function* for discrete target space $\\mathcal{T}$: $P(X = x)$  the probability that a random variable $X$ takes a particular value $x \\in \\mathcal{T}$.\n",
    "- *cumulative distribution function*  $P(X \\leq x)$: by convention to pecify the probability that a random variable $X$ is less than a particular value $x$. Generaly, we specify the probability that a random variable $X$ is in an interval, denoted by $P(a \\leq X \\leq b)$ for $a \\leq b$.\n",
    "- **Remark** :\n",
    "    - *univariate distribution* to refer to distributions of a single random variable.\n",
    "    - *multivariate distributions*: a vector of random variables\n",
    "### 6.2.1 Discrete Probabilities\n",
    "- Target space is discrete => The probabilty distribution of multiple random variables is same as filling out a (multidimensional) array of numbers.\n",
    "- *Joint probability*: The Cartesian product of the target spaces of each of the random variables.\n",
    "$$\n",
    "P(X = x_i, Y = y_j) = \\frac{n_{ij}}{N}\n",
    "$$\n",
    "Where $n_{ij}$ is the number of events with state $x_i$ and $y_j$ and $N$ the total\n",
    "number of events.\n",
    "\n",
    "or \n",
    "$$\n",
    "P(X = x_i, Y = y_j) = P(X = x_i \\cap Y = y_j)\n",
    "$$\n",
    "- For two random variables $X$ and $Y$ , the probability that $X = x$ and $Y = y$ is (lazily) written as $p(x, y)$ and is called the joint probability.\n",
    "- The *marginal probability* that $X$ takes the value $x$ irrespective of the value of random variable $Y$ is (lazily) written as $p(x)$. We write $X \\sim p(x)$ to denote that the random variable X is distributed according to $p(x)$.\n",
    "- *Conditional probabilty* : if we consider only $X=x$, the probabilty for $Y=y$ is written as $p(y|x)$.\n",
    "- Exemple : see p179\n",
    "- in ML, we use discrete probability distributions to model *categorical variables*.\n",
    "- Discrete distributions are also often used to construct probabilistic models that combine a finite number of continuous distributions\n",
    "### 6.2.2 Continuous Probabilities\n",
    "- real-valued random variables.target spaces are intervals of the real line R.\n",
    "- In this book, we pretend that we can perform operations on real random variables as if we have discrete probability spaces with finite states.\n",
    "- **Remark**: Continuous spaces have two additional technicalities (see p180)\n",
    "    - *measure*: The size of a set (ex:cardinality of discrete sets, length of an interval in $\\mathbb{R}$, colume of a region in $\\mathbb{R}^d$\n",
    "    - $Borel\\text{ }\\sigma-algebra$ : Sets that behave well under set operations and additionally have a topology (see Jacod, Jean, and Protter, Philip. 2004. Probability Essentials. Springer.)\n",
    "- In this book, we pick random variables with their corresponding $Borel\\text{ }\\sigma-algebra$, thus random variables are real-valued  vector in $\\mathbb{R}^D$\n",
    "- **Probability Density Function** (pdf) : a function $f : \\mathbb{R}^D \\to \\mathbb{R}$ where :\n",
    "    1. $\\forall x \\in \\mathbb{R}^D : f(x) \\geq 0$\n",
    "    2. Its integral exists and\n",
    "       $$\n",
    "       \\int_{R^D} f(x) \\, dx = 1 \\tag{6.15}\n",
    "       $$\n",
    "   For probability mass functions (pmf) of discrete random variables, the integral in (6.15) is replaced with a sum.\n",
    "- **the law or distribution of the random variable $X$** : We associate a random variable X with this function f by\n",
    "$$\n",
    "P( a \\leq X \\leq b) : \\int_a^b f(x) \\, dx , \\tag{6.16}\n",
    "$$\n",
    "Where $a,b \\in \\mathbb{R}$ and $x \\in \\mathbb{R}$ are outcomes of the continuous random variable $X$.\n",
    "**Remark**: $P (X = x)$ is a set of measure zero. This is like trying to specify an interval in (6.16) where a = b.\n",
    "- **Cumulative Distribution Function** (cdf) of a multivariate real-valued random variable $X$ with states $x \\in \\mathbb{R}^D$ is given by:\n",
    "  $$\n",
    "  F_{X}(x) = P(X_1 \\leq x_1, \\cdots ,X_D \\leq x_D), \\tag{6.17}\n",
    "  $$\n",
    "  where $X = \\left[X_1,\\cdots,X_D\\right]^T, \\, x=\\left[x_1,\\cdots,x_D\\right]^T$ and the right-hand side represents the probability that random variable Xi takes the value smaller than or equal to xi .\n",
    "- The cdf can be expressed also as the integral of the robability density function (pdf) f (x) so that\n",
    "  $$\n",
    "    F_X(x) = \\int_{-\\infty}^{x_1} \\cdots \\int_{-\\infty}^{x_D} f(z_1,\\cdots,z_D) \\, dz_1 \\cdots dz_D \\tag{6.18}\n",
    "  $$\n",
    "- **Remark** $f(x)$(pdf): is a nonnegative function that sums to one. There are cdfs, which do not have corresponding pdfs. *law of a random variable $X$*: the association of a random variable $X$ with the pdf $f(x)$.\n",
    "\n",
    "### 6.2.3 Contrasting Discrete and Continuous Distributions\n",
    "- probabilities are positive and the total probability sums up to one. thus , for discrete random variables, $ p \\in \\left[0, 1\\right]$ but for continuous random variables, the normalization does not imply that the value of the density is less than or equal to 1 for all values\n",
    "- **Example 6.3** (see p182)\n",
    "- **Remark**: The states $z_1 , . . . , z_d$ do not in principle have any structure, i.e., there is usually no way to compare them, for example\n",
    "$z_1 =$ red, $z_2 =$ green, $z_3 =$ blue. However, in many machine learning applications discrete states take numerical values, e.g., $z_1 = −1.1, z_2 = 0.3, z_3 = 1.5$, where we could say $z1 < z2 < z3$\n",
    "- In ML, there is no distinction of sample space $\\Omega$, the target\n",
    "space $\\mathcal{T}$ , and the random variable $X$.\n",
    "- $\\forall x \\in \\mathcal{T}$, $p(x)$ denotes the probability that random variable $X$ has the outcome $x$. for discrete Random Variable $p(x)=P(X = x)$ (the probabilty mass function); pmf: Distribution, for continuous variables, $p(x)$ is the pdf (density). cdf $P(X \\leq x)$ is also refered as Distributions\n",
    "- **Remark** : “probability distribution => for discrete probability mass functions & for continuous probability density functions, although this is technically incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb570ce-f27e-44e1-be5e-59674bdaeb58",
   "metadata": {},
   "source": [
    "## 6.3 Sum Rule, Product Rule, and Bayes’ Theorem\n",
    "Recall: joint distribution = $p(x,y)$, marginal distributions = $p(x)$ or $p(y)$; conditional distribution of y given x = $p(y|x)$. (see *Jaynes, Edwin T. 2003. Probability Theory: The Logic of Science. Cambridge University Press.*)\n",
    "1. **Sum rule** or **marginalization property**\n",
    "$$\n",
    "p(x) = \n",
    "\\begin{cases}\n",
    "\\sum_{y \\in \\mathcal{Y}} p(x,y), & \\text{if $y$ is discrete} \\\\[2mm]\n",
    "\\int_{\\mathcal{Y}} p(x,y) \\, dy, & \\text{if $y$ is continuous}\n",
    "\\end{cases}\n",
    "\\tag{6.20}\n",
    "$$ where $\\mathcal{Y}$ are the states of the target space of random variable $Y$.\n",
    "- The sum rule relates the joint distribution to a marginal distribution.\n",
    "- the sum rule can be applied to any subset of the random variables:\n",
    "\n",
    "$$\n",
    "p(x_i)= \\int p(x_1,\\cdots,x_D) \\, dx_{\\textbackslash i}  \\tag{6.21}\n",
    "$$\n",
    "we integrate/sum out all random variables except $x_i$\n",
    "\n",
    "- **Remark**: Sum rule is generally computationally hard because it performs high-dimensional sums or integral for my variables.\n",
    "2. **Product Rule** : relates the joint distribution to the conditional distribution via:\n",
    "  $$\n",
    "    p(x,y) = p(y | x) p(x) \\tag{6.22}\n",
    "  $$\n",
    "  - The product rule can be interpreted as the fact that every joint distribution of two random variables can be factorized (written as a product) of two other distributions.\n",
    "  $$\n",
    "    p(y,x) = p(x | y) p(y)\n",
    "  $$\n",
    "3. **Bayes Theorem** or *probabilistic inverse* : Bayes’ theorem is used to draw some conclusions about x given the observed values of y . allows us to invert the relationship between x and y given by the likelihood.\n",
    "    $$\n",
    "    \\underbrace{p(x|y)}_{\\text{posterior}} = \n",
    "    \\frac{\n",
    "      \\overbrace{p(y|x)}^{\\text{likelihood}} \\,\n",
    "      \\overbrace{p(x)}^{\\text{prior}}\n",
    "  }{ \\underbrace{p(y)}_{\\text{evidence}}}\n",
    "  \\tag{6.23}\n",
    "    $$\n",
    "\n",
    "- it is a direct consequence of the product rule in (6.22) since $p(x,y) = p(y,x)$\n",
    "- *prior*:  encapsulates our subjective prior knowledge of the unobserved (latent) variable x before observing any data.\n",
    "- *likelihood* : describes how x and y are related, and in the case of discrete probability distributions, it is the probability of the data y if we were to know the latent variable x. The likelihood is sometimes also called the “measurement model”. “probability of y given x”\n",
    "- *posterior* : the quantity of interest in Bayesian statistics because it expresses exactly what we are interested in, i.e., what we know about x after having observed y .\n",
    "- **marginal likelihood/evidence** :\n",
    "$$\n",
    "    p(y) = \\int p(y|x) p(x)\\,dx = \\mathbb{E}_X \\left[p(y|x)\\right] \\tag{6.27}\n",
    "$$\n",
    "    - the marginal likelihood is independent of $x$, and it ensures that the posterior $p(x | y)$ is normalized. The marginal likelihood can also be interpreted as the expected likelihood where we take the expectation with respect to the prior $p(x)$.\n",
    "- **Remark** : In Bayesian statistics, the posterior distribution is the quantity of interest as it encapsulates all available information from the prior and the data. having the full posterior can be very useful for a downstream task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf6a7f8-9d2d-4d37-8d67-2c8336a8af34",
   "metadata": {},
   "source": [
    "## 6.4 Summary Statistics and Independence\n",
    "summarizing sets of random variables and comparing pairs of random variables. useful view of how a random variable behaves\n",
    "### 6.4.1 Means and Covariances\n",
    "- useful to describe properties of probability distributions (expected values and spread)\n",
    "- The concept of the expected value is central to machine learning\n",
    "- the **Expected Value** (or *the law of the unconscious statistician*) of a function $g : \\mathbb{R} \\to\n",
    "\\mathbb{R}$ of a univariate continuous random variable $X \\sim p(x)$ is given by\n",
    "\n",
    "$$ \n",
    "\\mathbb{E} \\left[g(x)\\right] = \\int_{\\mathcal{X}} g(x) p(x) \\, dx \\tag{6.28}\n",
    "$$\n",
    "- Correspondingly, the *expected value* of a function $g$ of a discrete random variable $X \\sim p(x)$ is given by\n",
    "$$ \n",
    "\\mathbb{E} \\left[g(x)\\right] = \\sum_{x \\in \\mathcal{X}} g(x) p(x), \\tag{6.2}\n",
    "$$\n",
    "where $\\mathcal{X}$ is the set of possible outcomes (the target space) of the random variable $X$. In this section, we consider discrete random variables to have numerical outcomes.\n",
    "- **Remark** : We consider multivariate random variables $X$ as a finite vector of univariate random variables $\\left[X_1,\\cdots,X_D \\right]_⊤$. For multivariate random variables, we define the expected value element wise :\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_X \\left[g(x)\\right] = \n",
    "\\begin{bmatrix}\n",
    "\\mathbb{E}_{X_i} \\left[g(x_i)\\right] \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbb{E}_{X_D} \\left[g(x_D)\\right] \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^D \\tag {6.30}\n",
    "$$\n",
    "\n",
    "where the subscript $E_{X_d}$ indicates that we are taking the expected value with respect to the $d$th element of the vector $x$.\n",
    "\n",
    "The definition of the mean , is a special case of the expected value, obtained by choosing g to be the identity function.\n",
    "\n",
    "#### <u>Mean</u> : \n",
    "The mean of a random variable $X$ with states  $x \\in \\mathbb{R}^D$ is an average and is defined as\n",
    "$$\n",
    "\\mathbb{E}_X \\left[x\\right] = \n",
    "\\begin{bmatrix}\n",
    "\\mathbb{E}_{X_i} \\left[x_i\\right] \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbb{E}_{X_D} \\left[x_D\\right] \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^D \\tag {6.30}\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "E_{X_d}[x_d] := \n",
    "\\begin{cases}\n",
    "\\sum_{x_i \\in \\mathcal{X}} x_i p(x_d = x_i), & \\text{if $X$ is a discrete random variable} \\\\[2mm]\n",
    "\\int_{\\mathcal{X}} x_d p(x_d) \\, dx_d, & \\text{if $X$ is a continuous random variable}\n",
    "\\end{cases}\n",
    "\\tag{6.32}\n",
    "$$ \n",
    "for $d = 1,\\cdots, D$, where the subscript $d$ indicates the corresponding dimension of $x$. The integral and sum are over the states $\\mathcal{X }$of the target space of the random variable $X$ .\n",
    "\n",
    "#### <u>Median</u>\n",
    "he median is the “middle” value if we sort the values, i.e., 50% of the values are greater than the median and 50% are smaller than the median. For continuous Random Variable $cdf=0.5$ . the median is more robust to outliers than the mean.\n",
    "#### <u>Mode </u>\n",
    "the value of x having the highest frequency of occurrence. For a continuous random variable, the mode is defined as a peak in the density p(x). A particular density p(x) may have more than one mode, and furthermore there may be a very large number of modes in high-dimensional distributions.\n",
    "#### Exemple (see p188)\n",
    "- **Remark**: The *expected value* is a linear operator.\n",
    "#### Covariance (Univariate) :\n",
    "he covariance between two univariate random variables $X$, $Y \\in \\mathbb{R}$ is given by the expected product of their deviations from their respective means, i.e.\n",
    "$$\n",
    "Cov_{X,Y}\\left[x,y\\right] := \\mathbb{E}_{X,Y}\\left[(x-\\mathbb{E}_X\\left[x\\right])(y-\\mathbb{E}_Y\\left[y\\right] \\right] \\tag{6.35}\n",
    "$$\n",
    "\n",
    "The covariance intuitively represents the notion of how dependent random variables are to one another.\n",
    "**Remark** : \n",
    "- For multivariate random variables, $Cov\\left[x,y\\right]$ is called *cross-covariance* with covariance\n",
    "referring to $Cov\\left[x, x\\right]$.\n",
    "- When the random variable associated with the expectation or covariance is clear by its arguments, the subscript is often suppressed (for example, $E_X\\left[x\\right]$ is often written as $E\\left[x\\right]$).\n",
    "- By using the linearity of expectations, we get\n",
    "  $$\n",
    "Cov_{X,Y}\\left[x,y\\right] :=  \\mathbb{E}\\left[xy\\right] - \\mathbb{E}\\left[x\\right]\\mathbb{E}\\left[y\\right] \\tag{6.36}\n",
    "$$\n",
    "- **variance**: The covariance of a variable with itself $Cov\\left[x,x\\right] $\n",
    "- **Standard deviation**: The square root of the *variance*, $\\sigma (x)$\n",
    "#### Covariance (Multivariate) \n",
    "The notion of covariance generalized to multivariate random variables. \n",
    "If we consider two multivariate random variables $X$ and $Y$ with states $x \\in \\mathbb{R}^D$ and $yy \\in \\mathbb{R}^E$ respectively, the covariance between $X$ and $Y$ is defined as\n",
    "$$\n",
    "Cov\\left[x,y\\right] :=  \\mathbb{E}\\left[xy^T\\right] - \\mathbb{E}\\left[x\\right]\\mathbb{E}\\left[y\\right]^T \\in \\mathbb{R}^{D\\text{x}E} \\tag{6.37}\n",
    "$$\n",
    "\n",
    "For a multivariate random variable, the variance describes the relation between individual dimensions of the random variable.\n",
    "\n",
    "#### Variance \n",
    "The variance of a random variable $X$ with states $x \\in \\mathbb{R}^D$ and a mean vector $µ \\in \\mathbb{R}^D$ is defined as\n",
    "$$\n",
    "\\begin{align}\n",
    "V_X[x] &= \\mathrm{Cov}_X[x, x] \\tag{6.38a} \\\\[2mm]\n",
    "&= \\mathbb{E}_X \\big[(x - \\mu)(x - \\mu)^\\top \\big] \\notag \\\\[2mm]\n",
    "&= \\mathbb{E}_X[xx^\\top] - \\mathbb{E}_X[x]\\mathbb{E}_X[x]^\\top \\tag{6.38b} \\\\[4mm]\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\mathrm{Cov}[x_1, x_1] & \\mathrm{Cov}[x_1, x_2] & \\cdots & \\mathrm{Cov}[x_1, x_D] \\\\[2mm]\n",
    "\\mathrm{Cov}[x_2, x_1] & \\mathrm{Cov}[x_2, x_2] & \\cdots & \\mathrm{Cov}[x_2, x_D] \\\\[2mm]\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\[2mm]\n",
    "\\mathrm{Cov}[x_D, x_1] & \\mathrm{Cov}[x_D, x_2] & \\cdots & \\mathrm{Cov}[x_D, x_D]\n",
    "\\end{bmatrix}\n",
    "\\tag{6.38c}\n",
    "\\end{align}\n",
    "$$\n",
    "The $D \\text{×} D$ matrix in (6.38c) is called the covariance matrix of the multivariate random variable $X$ . it's a symmetric and positive semidefinite matrix and tells us something about the spread of the data. The variances of the marginals is in its diagonals : \n",
    "$$\n",
    "p(x_i) = \\int p(x_i,\\cdots,x_D) \\, dx_{\\textbackslash i},\n",
    "$$\n",
    "\n",
    "*cross-covariance* : The off-diagonal entries $Cov\\left[x_i,x_j\\right]$ for $i,j=1,\\cdots,D, i \\neq j$\n",
    "\n",
    "#### Correlation\n",
    "The correlation between two random variables X, Y is given by\n",
    "$$\n",
    "    corr\\left[x,y\\right] = \\frac{Cov\\left[x,y\\right]}{\\sqrt{\\mathbb{V}\\left[x\\right]\\mathbb{V}\\left[x\\right]}} \\in \\left[-1,1\\right]\n",
    "$$\n",
    "\n",
    "The correlation matrix is the covariance matrix of standardized random variables, $x/\\sigma (x)$.\n",
    "\n",
    "The covariance (and correlation) indicate how two random variables\n",
    "are related\n",
    "\n",
    "### 6.4.2 Empirical Means and Covariances\n",
    "In machine learning, we need to learn from empirical observations of data.\n",
    "$X$: a random variable\n",
    "Two steps: \n",
    "- with finite dataset (of size $N$), we can construct an empirical statistic that is a function of a finite number of identical random variables, $X_1 ,\\cdots, X_N$ .\n",
    "- we observe the data, that is, we look at the realization $x_1 ,\\cdots, x_N$ of each of the random variables and apply the empirical statistic.\n",
    "\n",
    "*empirical mean* or *sample mean*: an estimate of the mean (based on the mean of a particular dataset)\n",
    "\n",
    "#### Empirical Mean and Covariance\n",
    "The empirical mean vector is the arithmetic average of the observations for each variable, and it is defined as\n",
    "$$\n",
    "\\bar{x} := \\frac{1}{N} \\sum_{n=1}^{N} x_n\n",
    "$$\n",
    "where $x_n \\in \\mathbb{R}^D$\n",
    "\n",
    "Similar to the empirical mean, the empirical covariance matrix is a $D×D$\n",
    "matrix \n",
    "$$\n",
    "\\Sigma := \\frac{1}{N} \\sum_{n=1}^{N} \\left( x_n - \\bar{x} \\right) \\left( x_n - \\bar{x} \\right)^{\\top}, \n",
    "\\quad x_n \\in \\mathbb{R}^D \\tag{6.42}\n",
    "$$\n",
    "\n",
    "Empirical covariance matrices are symmetric, positive semidefinite.\n",
    "Throughout the book, we use the\n",
    "empirical covariance, which is\n",
    "a biased estimate. The unbiased\n",
    "(sometimes called corrected)\n",
    "covariance has the factor N − 1 in the\n",
    "denominator instead of N .\n",
    "### 6.4.3 Three Expressions for the Variance\n",
    "We now focus on a single random variable $X$\n",
    "The standard definition of variance, corresponding to the definition of covariance, is the expectation of the squared deviation of a random variable X from its expected value µ, i.e.,\n",
    "$$\n",
    "V_X[x] := \\mathbb{E}_X \\left[ (x - \\mu)^2 \\right] \\tag{6.43}\n",
    "$$\n",
    "The expectation in (6.43) and the mean $\\mu = \\mathbb{E}_X \\left[ (x) \\right]$ are computed using (6.32). The variance as expressed in (6.43) is the mean of a new random variable $Z := (X − µ)^2$.\n",
    "Two-pass algorithm for estimating the variance in (6.43).\n",
    "- one pass through the data to calculate the mean $\\mu$ using (6.41)\n",
    "- a second pass using this estimate $\\hat{\\mu}$ calculate the variance.\n",
    "\n",
    "**Raw-score formula for variance** : “the mean of the square minus the square of the mean”\n",
    "$$\n",
    "V_X[x] = \\mathbb{E}_X[x^2] - \\big(\\mathbb{E}_X[x]\\big)^2 \\tag{6.44}\n",
    "$$\n",
    "If the two terms in (6.44) are huge and approximately equal, we may suffer from an unnecessary loss of numerical precision in floating-point arithmetic.\n",
    "\n",
    "The raw-score version of the variance can be useful in machine learning, e.g., when deriving the bias–variance decomposition (Bishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.).\n",
    "\n",
    "A third way to understand the variance is that it is a sum of pairwise differences between all pairs of observations. Consider a sample $x_1,\\cdot,x_N$ of realizations of random variable X and we compute the squared difference between pairs of xi and xj .\n",
    "$$\n",
    "\\frac{1}{N^2} \\sum_{i,j=1}^N (x_i - x_j)^2 \n",
    "= 2 \\left[ \\frac{1}{N} \\sum_{i=1}^N x_i^2 -  \\left( \\frac{1}{N} \\sum_{i=1}^N x_i \\right)^2 \\right].\n",
    "\\tag{6.45}\n",
    "$$\n",
    "\n",
    "We see that (6.45) is twice the raw-score expression (6.44). This means that we can express the sum of pairwise distances (of which there are $N^2$ of them) as a sum of deviations from the mean (of which there are $N$ ).\n",
    "\n",
    "\n",
    "### 6.4.4 Sums and Transformations of Random Variables\n",
    "Consider two random variables $X, Y$ with states $x, y \\in \\mathbb{R}^D$ . Then:\n",
    "$$\n",
    "\\mathbb{E} \\left[ x + y  \\right] = \\mathbb{E} \\left[ x \\right] + \\mathbb{E} \\left[ y  \\right] \\tag{6.46} \n",
    "$$\n",
    "$$\n",
    "\\mathbb{E} \\left[ x - y  \\right] = \\mathbb{E} \\left[ x \\right] - \\mathbb{E} \\left[ y  \\right] \\tag{6.47} \n",
    "$$\n",
    "$$\n",
    "\\mathbb{V} \\left[ x + y  \\right] = \\mathbb{V} \\left[ x \\right] + \\mathbb{V} \\left[ y  \\right] + Cov \\left[ x,y \\right] + Cov \\left[ y,x \\right] \\tag{6.48} \n",
    "$$\n",
    "$$\n",
    "\\mathbb{V} \\left[ x - y  \\right] = \\mathbb{V} \\left[ x \\right] + \\mathbb{V} \\left[ y  \\right] - + Cov \\left[ x,y \\right] - Cov \\left[ y,x \\right] \\tag{6.49}  \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "Mean and (co)variance exhibit some useful properties when it comes to affine transformation of random variables.Consider a random variable\n",
    "$X$ with mean $\\mu$ and covariance matrix $\\Sigma$ and a (deterministic) affine\n",
    "transformation $y = Ax + b$ of $x$. Then $y$ is a also a random variable whose mean vector and covariance matrix are given by\n",
    "$$\n",
    "\\mathbb{E}_{\\mathcal{Y}} \\left[ y  \\right] = \\mathbb{E}_{\\mathcal{X}} \\left[ Ax + b \\right] = A\\mathbb{E}_{\\mathcal{X}} \\left[ x  \\right] + b = A \\mu + b \\tag{6.50}\n",
    "$$\n",
    "$$\n",
    "\\mathbb{V}_{\\mathcal{Y}} \\left[ y  \\right] = \\mathbb{V}_{\\mathcal{X}} \\left[ Ax + b \\right] = \\mathbb{V}_{\\mathcal{X}} \\left[ A x  \\right]  = A \\mathbb{V}_{\\mathcal{X}} \\left[  x  \\right] A^T = A  \\Sigma A^T\n",
    "$$\n",
    "\n",
    "respectiveley. Futhermore\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{Cov}[x, \\, Ax + b] \n",
    "&= \\mathbb{E}\\!\\left[ x (Ax + b)^\\top \\right] - \\mathbb{E}[x] \\, \\mathbb{E}[Ax + b]^\\top \\tag{6.38b} \\\\[6pt]\n",
    "&= \\mathbb{E}[x] b^\\top + \\mathbb{E}[xx^\\top] A^\\top - \\mu b^\\top - \\mu \\mu^\\top A^\\top \\tag{6.52a} \\\\[6pt]\n",
    "&= \\mu b^\\top - \\mu b^\\top + \\mathbb{E}[xx^\\top] A^\\top - \\mu \\mu^\\top A^\\top \\tag{6.52b} \\\\[6pt]\n",
    "&= \\big(\\mathbb{E}[xx^\\top] - \\mu \\mu^\\top \\big) A^\\top \\tag{6.52c} \\\\[6pt]\n",
    "&= \\Sigma A^\\top \\tag{6.52d}\n",
    "\\end{align}\n",
    "$$\n",
    "Where $\\Sigma = \\mathbb{E}[xx^\\top] - \\mu \\mu^\\top$ is the covariance of X.\n",
    "### 6.4.5 Statistical Independence\n",
    "#### (Independence)\n",
    "Two random variables X, Y are statistically independent if and only if\n",
    "$$\n",
    "p(x, y) = p(x) \\, p(y) .\n",
    "\\tag{6.53}\n",
    "$$\n",
    "1. $p(y \\mid x) = p(y)$  \n",
    "\n",
    "2. $p(x \\mid y) = p(x)$  \n",
    "\n",
    "3. $V_{X,Y}[x+y] = V_X[x] + V_Y[y]$  \n",
    "\n",
    "4. $\\operatorname{Cov}_{X,Y}[x, y] = 0$\n",
    "\n",
    "The last point may not hold in converse, i.e., two random variables can have covariance zero but are not statistically independent.\n",
    "#### Exemple 6.5 (see p194)\n",
    "\n",
    "#### Independent and identically distributed (i.i.d.) random variables $X_1,\\cdots,X_N$\n",
    "modeling problem in ML like this.\n",
    "For more than two random variables, the word “**independent**” usually refers to mutually independent random variables, where all subsets are independent\n",
    "The phrase “**identically distributed**” means that all the random variables are from the same distribution.\n",
    "\n",
    "#### Conditional Independence\n",
    "Two random variables $X$ and $Y$ are conditionally independent given $Z$  (noted $X \\;\\perp\\!\\!\\!\\perp\\; Y \\;\\mid\\; Z$ ) if and only if\n",
    "$$\n",
    "p(x, y \\mid z) = p(x \\mid z) \\, p(y \\mid z), \\quad \\forall z \\in \\mathcal{Z} \\tag{6.55}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{Z}$ is the set of states of the random variable $Z$.\n",
    "This is his expansion: \n",
    "\n",
    "$$\n",
    "p(x, y \\mid z) = p(x \\mid y, z) \\, p(y \\mid z) \\tag{6.56}\n",
    "$$\n",
    "\n",
    "From (6.55) with (6.56), we get : \n",
    "$$\n",
    "p(x \\mid y, z) = p(x \\mid z) \\tag{6.57}\n",
    "$$\n",
    "This means  : “given that we know z , knowledge about y does not change our knowledge of x”\n",
    "\n",
    "### 6.4.6 Inner Products of Random Variables\n",
    "If we have two uncorrelated random variables X, Y , then\n",
    "$$ \n",
    "\\mathbb{V}[x + y] = \\mathbb{V}[x] + \\mathbb{V}[y] \\tag{6.58}\n",
    "$$\n",
    "Random variables can be considered vectors in a vector space, and we can define inner products to obtain geometric properties of random variables. Be the inner product where zero mean random variables X and Y:\n",
    "$$\n",
    "\\langle X, Y \\rangle := \\operatorname{Cov}[x, y] \\tag{6.59}\n",
    "$$\n",
    "We see that the covariance is symmetric, positive definite, and linear in either argument. The length of a random variable is \n",
    "$$\n",
    "\\|X\\| = \\sqrt{\\operatorname{Cov}[x, x]} = \\sqrt{V[x]} = \\sigma[x] \\tag{6.60}\n",
    "$$\n",
    "(The standard deviation)\n",
    "The “longer” the random variable, the more uncertain it is; and a random variable with length 0 is deterministic.\n",
    "\n",
    "Angle $\\theta$ between two random variables $X,Y$:\n",
    "\n",
    "$$\n",
    "\\cos \\theta = \\frac{\\langle X, Y \\rangle}{\\|X\\| \\, \\|Y\\|} \n",
    "= \\frac{\\operatorname{Cov}[x, y]}{\\sqrt{V[x]}\\, \\sqrt{V[y]}}\n",
    "$$\n",
    " (the correlation between the two random varibles)\n",
    " $$\n",
    "X \\perp Y \\;\\; \\Longleftrightarrow \\;\\; \\langle X, Y \\rangle = 0 \n",
    "\\;\\; \\Longleftrightarrow \\;\\; \\operatorname{Cov}[x, y] = 0 \\;\\; \\Longleftrightarrow \\;\\;\\text{ they are uncorrelated }\n",
    "$$\n",
    "\n",
    "**Remark**: the euclidean distance is not the best way to obtain distances between distributions (see P 197 for a statistical manifold or information geometry). It is done using Kullback-Leibler divergence, hich is a generalization of distances that account for properties of the statistical manifold. To see more about that, look up for : *Amari, Shun-ichi. 2016. Information Geometry and Its Applications. Springer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05336b24-4a65-45c8-909c-7ff5f24bbda4",
   "metadata": {},
   "source": [
    "## 6.5. Gaussian Distribution\n",
    "the most well-studied probability distribution for continuous-valued random variables.\n",
    "*Normal Distribution*. \n",
    "\n",
    "The Gaussian distribution arises naturally when we consider sums of independent and identically distributed random variables. This is known as the **central limit theorem** (see Grinstead, Charles M., and Snell, J.Laurie. 1997. Introduction to Probability. American Mathematical Society.)\n",
    "\n",
    "widely used in statistical estimation and machine learning as they have closed-form expressions for marginal and conditional distributions. ML areas that profit frmo Gaussian distribution: Gaussian processes, variational inference, and reinforcement learning. also in signal processing, contrl, and statistics\n",
    "\n",
    "- For a univariate random variable, the Gaussian distribution has a density that is given by\n",
    "  $$\n",
    "p(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \n",
    "\\exp \\Bigg( - \\frac{(x - \\mu)^2}{2 \\sigma^2} \\Bigg) \n",
    "\\tag{6.62}\n",
    "$$\n",
    "The *multivariate Gaussian distribution* is fully characterized by a *mean vector* $\\mu$ and a *covariance matrix* $\\sigma$ and defined as\n",
    "$$\n",
    "p(x \\mid \\mu, \\Sigma) = \\frac{1}{(2 \\pi)^{-D/2} |\\Sigma|^{-1/2}} \n",
    "\\exp \\Bigg( -\\frac{1}{2} (x - \\mu)^\\top \\Sigma^{-1} (x - \\mu) \\Bigg)\n",
    "$$\n",
    "where $x \\in \\mathbb{R}^D$. We write $p(x) = \\mathcal{N}(x \\mid \\mu, \\Sigma$ or $X \\sim \\mathcal{N} (\\mu, \\Sigma)$\n",
    "- *the standard normal distribution* : $\\mu = 0$ and $\\Sigma = I$\n",
    "- When modeling with Gaussian random variables, variable transformations (Section 6.7) are often not\n",
    "needed. Since the Gaussian distribution is fully specified by its mean and covariance, we often can obtain the transformed distribution by applying the transformation to the mean and covariance of the random variable.\n",
    "\n",
    "\n",
    "### 6.5.1 Marginals and Conditionals of Gaussians are Gaussians\n",
    "(in the general case of multivariate random variables)\n",
    "Let X and Y be two multivariate random variables, that may have different dimensions. \n",
    "The Gaussian distribution in terms of the concatenated states $[x^\\top \\, y^\\top ]^\\top$ so that\n",
    "$$\n",
    "p(x, y) =\n",
    "\\mathcal{N} \\Bigg(\n",
    "\\begin{bmatrix} \n",
    "\\mu_x \\\\\n",
    "\\mu_y\n",
    "\\end{bmatrix}, \n",
    "\\begin{bmatrix} \n",
    "\\Sigma_{xx} & \\Sigma_{xy} \\\\\n",
    "\\Sigma_{yx} & \\Sigma_{yy}\n",
    "\\end{bmatrix} \n",
    "\\Bigg)\n",
    "\\tag{6.64}\n",
    "$$\n",
    "\n",
    "Where $\\Sigma_{xx} =  \\operatorname{Cov}[x,x]$ and $\\Sigma_{yy} =  \\operatorname{Cov}[y,y]$ are the marginal covariance matrices of x and y , respectively, and $\\Sigma_{xy} =  \\operatorname{Cov}[x,y]$  is the cross-covariance matrix between x and y .\n",
    "The conditional distribution p(x | y) is also Gaussian and given by\n",
    "$$\n",
    "p(x \\mid y) = \\mathcal{N}(\\mu_{x \\mid y}, \\Sigma_{x \\mid y}) \\tag{6.65}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_{x \\mid y} = \\mu_x + \\Sigma_{xy} \\, \\Sigma_{yy}^{-1} (y - \\mu_y) \\tag{6.66}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_{x \\mid y} = \\Sigma_{xx} - \\Sigma_{xy} \\, \\Sigma_{yy}^{-1} \\Sigma_{yx} \\tag{6.67}\n",
    "$$\n",
    "in (6.66), the $y$-value is an observation and no longer random.\n",
    "**Remark**: il find it in :\n",
    "- The Kalman filter\n",
    "- Gaussian processes (Rasmussen and Williams, 2006)\n",
    "- Latent linear Gaussian models (Roweis and Ghahramani, 1999; Murphy, 2012)\n",
    "\n",
    "####  The marginal distribution $p(x)$\n",
    "of a joint Gaussian distribution $p(x, y)$ is itself Gaussian\n",
    "$$\n",
    "p(x) = \\int p(x, y) \\, dy = \\mathcal{N}(x \\mid \\mu_x, \\Sigma_{xx}) \\tag{6.68}\n",
    "$$\n",
    "The corresponding result holds for p(y)\n",
    "**Exemple (on p200)**\n",
    "\n",
    "### 6.5.2 Product of Gaussian Densities\n",
    "The product of two Gaussians $\\mathcal{N} (x \\mid a, A) \\mathcal{N} (x \\mid b, B)$ is a Gaussian distribution scaled by a $c \\in \\mathbb{R}$, given by $c \\mathcal{N}(x \\mid c, C)$  with\n",
    "$$\n",
    "    C = (A^{-1} + B^{-1})^-1 \\tag{6.74}\n",
    "$$\n",
    "$$\n",
    "    c = C ( A^{-1} a + B ^{-1} b) \\tag{6.75}\n",
    "$$\n",
    "$$\n",
    "c = (2\\pi)^{- \\frac{D}{2} }|A + B|^{-1/2} \\exp \\Bigg[ -\\frac{1}{2} (a - b)^\\top (A + B)^{-1} (a - b) \\Bigg] \\tag{6.76}\n",
    "$$\n",
    "The scaling constant $c$ itself can be written in the form of a Gaussian density either in $a$ or in \u0001$b$ with an “inflated”\u0001 covariance matrix $A + B$ ,\n",
    "i.e., $c = \\mathcal{N} (a | b, A + B )= \\mathcal{N}(b | a, A + B)$\n",
    "\n",
    "**Remark.** For notation convenience, we will sometimes use $\\mathcal{N} (x | m, S )$ to describe the functional form of a Gaussian density even if $x$ is not a random variable.\n",
    "\n",
    "### 6.5.3 Sums and Linear Transformations\n",
    "$p(x) = \\mathcal{N} (x | \\mu_x, \\Sigma_x )$ and $p(y) = \\mathcal{N} (y | \\mu_y, \\Sigma_y )$\n",
    "\n",
    "$X, Y$ are independent Gaussian random variables $\\Longleftrightarrow$ $p(x,y) = p(x)p(y)$  $\\Longleftrightarrow$ $x + y$ is also Gaussian distributed and given by \n",
    "$$\n",
    "    p(x+y) = \\mathcal{N}(\\mu_x + \\mu_y, \\Sigma_x + \\Sigma_y)\n",
    "$$\n",
    "Knowing that p(x + y) is Gaussian, the mean and covariance matrix can be determined immediately using the results from (6.46) through (6.49). This notion is imoprt for *Gaussian noise* acting on random variables.\n",
    "\n",
    "#### Exemple 6.7 (p201) \n",
    "\n",
    "**Remark**: **Weighted sum of Gaussian densities** is very import for Chapter 11. \n",
    "**Theorem 6.12**. *Consider a mixture of two univariate Gaussian densities*\n",
    "$$\n",
    "p(x) = \\alpha p_1(x) + (1 -\\alpha)p_2(x) \\tag{6.80}\n",
    "$$\n",
    "where the scalar $ 0 < \\alpha < 1$  *is the mixture weight, and* $p_1 (x)$ and $p_2 (x)$ *are\n",
    "univariate Gaussian densities (Equation (6.62)) with different parameters, i.e.,* $(\\mu_1 , \\sigma_1^2 ) \\neq (\\mu_2 , \\sigma_2^2 )$.\n",
    "\n",
    "the random variable x is from a density that is a mixture of two densities $p_1 (x)$ and $p_2 (x)$, weighted by $\\alpha$.\n",
    "*Then the mean of the mixture density $p(x)$ is given by the weighted sum\n",
    "of the means of each random variable:* \n",
    "$$\n",
    "\\mathbb{E}[x] = \\alpha \\mu_1 + (1 - \\alpha) \\mu_2 \\tag{6.81}\n",
    "$$\n",
    "*The variance of the mixture density $p(x)$ is given by*\n",
    "$$\n",
    "\\mathbb{V}[x] = \\alpha \\sigma_1^2 + (1 - \\alpha) \\sigma_2^2 + \\alpha \\mu_1^2 + (1 - \\alpha) \\mu_2^2 - \\big[\\alpha \\mu_1 + (1 - \\alpha)\\mu_2\\big]² \\tag{6.82}\n",
    "$$\n",
    "*Proof* : See **p202**\n",
    "\n",
    "**Remark**: The preceding derivation holds for any density, but since the Gaussian is fully determined by the mean and variance, the mixture density can be determined in closed form.\n",
    "\n",
    "\n",
    "*<u>Law of total variance</u>* (conditional variance formula) : generally states that for two random variables X and Y it holds that $\\mathbb{V}[X] = \\mathbb{E}_Y \\big[\\mathbb{V}[X \\mid Y]\\big] + \\mathbb{V}_Y \\big[\\mathbb{E}[X \\mid Y]\\big]$ \n",
    "i.e., the (total) variance of X is the expected conditional variance plus the variance of a conditional mean.\n",
    "#### For linear transformation\n",
    "Any linear/affine transformation $A$ of a Gaussian random variable $x$ is also Gaussian distributed. The outcome is a Gaussian random variable with mean zero and covariance $AA^\\top$.\n",
    "\n",
    "$\\Longrightarrow$ the random variable x + µ is Gaussian with mean µ and identity covariance.\n",
    "\n",
    "For $X \\sim \\mathcal{N}(\\mu,\\Sigma)$, a given matrix $A$ and $Y$ a random variable such that $y = Ax$, we got :\n",
    "- *The mean* of $y$: $\\mathbb{E}[y] = \\mathbb{E}[Ax] = A\\mathbb{E}[x] = A\\mu \\tag{6.86}$\n",
    "\n",
    "- *The variance* of $y$ : $\\mathbb{V}[y] = \\mathbb{V}[Ax] = A \\mathbb{V}[x] A^\\top = A \\Sigma A^\\top \\tag{6.87}$\n",
    "\n",
    "- This means that the random variable y is distributed according to : $p(y) = \\mathcal{N}\\big(y \\mid A\\mu,\\; A\\Sigma A^\\top \\big) \\tag{6.88}$\n",
    "\n",
    "#### Reverse transformation:\n",
    "when we know that a random variable has a mean that is a linear transformation of another\n",
    "random variable.\n",
    "$A \\in \\mathbb{R}^{MxN}$ : A full rank matrix where $M \\geq N$\n",
    "$y \\in \\mathbb{R}^M$ : a *Gaussian random variable* with mean $AX), i.e, \n",
    "$$\n",
    "p(y) = \\mathcal{N} ( y  \\mid Ax, \\Sigma ) \\tag{6.89}\n",
    "$$\n",
    "\n",
    "$\\Longrightarrow$ he corresponding probability distribution $p(x)$: \n",
    "\n",
    "$$\n",
    "y = AX \\Longleftrightarrow (A^\\top A)^{-1} A^\\top y = x \\tag{6.90}\n",
    "$$\n",
    "Hence, x is a linear transformation of y , and we obtain : \n",
    "$$\n",
    "p(x) = \\mathcal{N}\\!\\left(x \\;\\middle|\\; (A^\\top A)^{-1} A^\\top y,\\; (A^\\top A)^{-1} A^\\top \\Sigma A (A^\\top A)^{-1} \\right) \\tag{6.91}\n",
    "$$\n",
    "\n",
    "### 6.5.4 Sampling from Multivariate Gaussian Distributions (p204)\n",
    "In the case of a multivariate Gaussian, this process consists of three stages:\n",
    "1. we need a source of pseudo-random numbers that provide a uniform sample in the interval $[0,1]$\n",
    "2. we use a non-linear transformation such as the Box-Müller transform (Devroye, 1986) to obtain a sample from a univariate Gaussian\n",
    "3. we collate a vector of these\u0001samples to obtain a sample from a multivariate standard normal $\\mathcal{N}(0, I)$ .\n",
    "general multivariate Gaussian $\\Longrightarrow$ mean is non zero and the covariance is not the identity matrix.\n",
    "\n",
    "To obtain samples from a multivariate normal $\\mathcal{N}(\\mu, \\Sigma)$, we can use\n",
    "the properties of\u0001 a linear transformation of a Gaussian random variable:  If $x \\sim \\mathcal{N}(0, I)$ , then $y = Ax + µ$, where $AA^⊤ = \\Sigma$ is Gaussian distributed with mean $\\mu$ and covariance matrix $\\Sigma$. One convenient choice of $A$ is to use the Cholesky decomposition  of the covariance matrix $\\Sigma = AA^⊤$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15116c86-705d-4d24-b95b-1630df8f2948",
   "metadata": {},
   "source": [
    "## 6.6. Conjugacy and the Exponential Family\n",
    "- “named” probability distributions: Probability distributions that are used to model particular types of phenoma\n",
    "- Each model aer related to each other in complex ways (Leemis, Lawrence M., and McQueston, Jacquelyn T. 2008. Univariate Distribution Relationships. American Statistician, 62(1), 45–53.\n",
    "- Efron, Bradley, and Hastie, Trevor. 2016. Computer Age Statistical Inference: Algorithms, Evidence and Data Science. Cambridge University Press.\n",
    "- desiderata for manipulating probability distributions in ML:\n",
    "1. “closure property” when applying the rules of probability\n",
    "2. As we collect more data, we do not need more parameters to describe the distribution.\n",
    "3. Since we are interested in learning from data, we want parameter estimation to behave nicely.\n",
    "\n",
    "- **exponential family** : provides the right balance of generality while retaining favorable computation and inference properties.\n",
    "- **Exemple 6.8** (See p205) where it explains the Bernouilli and Binomial and Beta distributions\n",
    "\n",
    "The Bernoulli distribution $\\text{Ber}(\\mu)$ is defined as\n",
    "$$\n",
    "p(x \\mid \\mu) = \\mu^x (1 - \\mu)^{1-x}, \\quad x \\in \\{0,1\\}\n",
    "$$\n",
    "\n",
    "with expectation and variance\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[x] = \\mu, \\qquad \\mathbb{V}[x] = \\mu(1 - \\mu).\n",
    "$$\n",
    "where $\\mathbb{E}[x]$ and  $\\mathbb{V}[x]$ are the mean and variance of the binary random variable $X$.\n",
    "**Remark** : the Bernouilli distribution is sometimes express in the exponents in machine learning textbooks (a trick)\n",
    "\n",
    "The Binomial distribution $\\text{Bin}(N, \\mu)$ is defined as\n",
    "\n",
    "$$\n",
    "p(m \\mid N, \\mu) = \\binom{N}{m} \\, \\mu^m (1 - \\mu)^{\\,N-m}, \n",
    "\\quad m \\in \\{0,1,\\dots,N\\}\n",
    "$$\n",
    "\n",
    "with expectation and variance of m\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[m] = N\\mu, \n",
    "\\qquad \n",
    "\\mathrm{Var}[m] = N\\mu(1 - \\mu).\n",
    "$$\n",
    "\n",
    "\n",
    "The **Beta distribution** $\\text{Beta}(\\alpha, \\beta)$ is a distribution over a continuous random variable $\\mu \\in [0,1]$.  \n",
    "It is defined as\n",
    "\n",
    "$$\n",
    "p(\\mu \\mid \\alpha, \\beta) \n",
    "= \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\,\\Gamma(\\beta)} \n",
    "\\, \\mu^{\\alpha - 1} (1 - \\mu)^{\\beta - 1}, \n",
    "\\quad \\alpha > 0, \\, \\beta > 0\n",
    "$$\n",
    "\n",
    "where $\\Gamma(\\cdot)$ is the Gamma function:\n",
    "\n",
    "$$\n",
    "\\Gamma(t) = \\int_0^\\infty x^{t-1} e^{-x} \\, dx, \n",
    "\\qquad t > 0\n",
    "$$\n",
    "\n",
    "with the recurrence property\n",
    "\n",
    "$$\n",
    "\\Gamma(t+1) = t \\, \\Gamma(t).\n",
    "$$\n",
    "\n",
    "\n",
    "The expectation and variance of $\\mu$ are\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\mu] = \\frac{\\alpha}{\\alpha + \\beta}, \n",
    "\\qquad\n",
    "\\mathbb{V}[\\mu] = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}.\n",
    "$$\n",
    "\n",
    "\n",
    "Intuitively:  \n",
    "- $\\alpha$ déplace la masse de probabilité vers **1**  \n",
    "- $\\beta$ déplace la masse de probabilité vers **0**  \n",
    "\n",
    "Cas particuliers :  \n",
    "- $\\alpha = 1, \\, \\beta = 1 \\;\\;\\Rightarrow\\;\\; U[0,1]$ (uniforme)  \n",
    "- $\\alpha, \\beta < 1 \\;\\;\\Rightarrow\\;\\;$ bimodale (pics en 0 et 1)  \n",
    "- $\\alpha, \\beta > 1 \\;\\;\\Rightarrow\\;\\;$ unimodale  \n",
    "- $\\alpha = \\beta > 1 \\;\\;\\Rightarrow\\;\\;$ unimodale, symétrique, centrée en $[0,1]$  \n",
    "see Leemis, Lawrence M., and McQueston, Jacquelyn T. 2008. Univariate Distribution Relationships. American Statistician, 62(1), 45–53.\n",
    "\n",
    "Each distribution is created for particular reason, that reason may be considered when choosing distribution\n",
    "\n",
    "### 6.6.1 Conjugacy (p208)\n",
    "#### Conjugate Prior\n",
    "A prior is conjugate for the likelihood function if the posterior is of the same form/type as the prior.\n",
    "\n",
    "In **Conjugacy**, we can algebraically calculate our posterior distribution by updating the parameters of the prior distribution.\n",
    "\n",
    "**Remark** : When considering the geometry of probability distributions, conjugate priors retain the same distance structure as the likelihood Agarwal, Arvind, and Daumé III, Hal. 2010. A Geometric View of Conjugate Priors. Machine Learning, 81(1), 99–113.\n",
    "\n",
    "**Example 6.11** (Beta-Binomial Conjugacy) : See p208\n",
    "**Example 6.12** (Beta-Bernoulli Conjugacy)\n",
    "\n",
    "The Gamma prior is conjugate for the precision (inverse variance) in the univariate Gaussian likelihood, and the Wishart prior is conjugate for the precision matrix (inverse covariance matrix) in the multivariate Gaussian likelihood.\n",
    "\n",
    "### 6.6.2 Sufficient Statistics\n",
    "**sufficient statistics**:the idea that there are statistics that will contain all available information that can be inferred from data corresponding to the distribution under consideration. (carry all the information needed to make inference about the population as a representation of the distribution) \n",
    "\n",
    "Set of distributions parametrized by $\\theta$. $X$: Random variable with distribution $p(x \\mid \\theta_0 )$ given an unknown \\theta_0.\n",
    "\n",
    "A vector $\\phi(x)$ of statistics is called **sufficient statistics** for $\\theta_0$ if it contains all possible information about $\\theta_0$.\n",
    "\n",
    "“contain all possible information” $\\Longleftrightarrow$  the probability of x given θ can be factored into a part that does not depend on θ, and a part that depends on θ only via ϕ(x).\n",
    "\n",
    "#### Fisher-Neyman (Theorem 6.5 in Lehmann, Erich Leo, and Casella, George. 1998. Theory of Point Estimation. Springer.)\n",
    "Let $X$ have probability density function $p(x \\mid \\theta)$. The statistics $\\phi(x)$ are **sufficient** for $\\theta$ if and only if $p(x \\mid \\theta)$ can be written in the form\n",
    "\n",
    "$$\n",
    "p(x \\mid \\theta) = h(x)\\, g_\\theta(\\phi(x)), \\tag{6.106}\n",
    "$$\n",
    "\n",
    "where $h(x)$ is a distribution independent of $\\theta$ and $g_\\theta$ captures all dependence on $\\theta$ via the sufficient statistics $\\phi(x)$.\n",
    "\n",
    "If  $p(x \\mid \\theta)$ does not depend on $\\theta$, then $\\phi(x)$ is trivially a sufficient statistic for any function $\\phi$. The more interesting case is that $p(x \\mid \\theta)$ is dependent only on $\\phi(x)$ and not $x$ itself. In this case, $\\phi(x)$ is a sufficient statistic for $\\theta$.\n",
    "\n",
    "In machine learning, we consider a finite number of samples from a distribution.\n",
    "(see Wasserman, Larry. 2007. All of Nonparametric Statistics. Springer.)\n",
    "\n",
    "### 6.6.3 Exponential Family\n",
    "Possible level when considering distributions (of continuous or discrete random variables)\n",
    "1. we have a particular named distribution with fixed parameters, for example a univariate Gaussian N 0, 1 with zero mean and unit variance.\n",
    "2. we fix the parametric form (the univariate Gaussian) and infer the parameters from data. For example, we assume a univariate Gaussian $\\mathcal{N}( \\mu, \\sigma^2)$ with unknown mean $\\mu$ and unknown variance $\\sigma^2$ , and use a maximum likelihood fit to determine the best parameters $(\\mu, \\sigma^2 )$\n",
    "3. to consider families of distributions, and in this book, we consider the exponential family. The univariate Gaussian is an example of a member of the exponential family.\n",
    "\n",
    "\n",
    "#### Definition\n",
    "An **exponential family** is a family of probability distributions, parameterized by $\\theta \\in \\mathbb{R}^D$, of the form\n",
    "\n",
    "$$\n",
    "p(x \\mid \\theta) = h(x) \\, \\exp \\Big( \\langle \\theta, \\phi(x) \\rangle - A(\\theta) \\Big), \\tag{6.107}\n",
    "$$\n",
    "\n",
    "where $\\phi(x)$ is the vector of **sufficient statistics**.\n",
    "In general, any inner product (Section 3.2) can be used in the exponential family formula.  \n",
    "For concreteness, we use the standard dot product here: $\\langle \\theta, \\phi(x) \\rangle = \\theta^\\top \\phi(x).$\n",
    "\n",
    "\n",
    "essentially a particular expression of $g_{\\theta} (\\phi(x))$in the Fisher-Neyman theorem (Theorem 6.14).\n",
    "\n",
    "**The log-partition function** : result of  the sum of the distributions when it is normalized with constant the terme $A(\\theta)$\n",
    "\n",
    "Exponential families can be considered as distributions of the form \n",
    "$$\n",
    "p(x \\mid \\theta) \\propto \\exp \\big( \\theta^\\top \\phi(x) \\big). \\tag{6.108}\n",
    "$$\n",
    "$\\theta$ : The *natural parameters*\n",
    "We can transform (6.108) for convenient modeling and efficient computation based on the fact that we can capture information about data in ϕ(x).\n",
    "#### Example 6.13 (Gaussian as Exponential Family) p212\n",
    "The **univariate Gaussian distribution** is a member of the exponential family with sufficient statistic\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\begin{bmatrix} x \\\\ x^2 \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and natural parameters\n",
    "\n",
    "$$\n",
    "\\theta = \\begin{bmatrix} \\frac{\\mu}{\\sigma^2} \\\\ -\\frac{1}{2\\sigma^2} \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "#### Example 6.14 (Bernoulli as Exponential Family)\n",
    "The **Bernoulli distribution** can be written in **exponential family** form:\n",
    "\n",
    "- Sufficient statistic: $\\phi(x) = x$  \n",
    "- Natural parameter: $\\theta = \\log \\frac{\\mu}{1-\\mu}$  \n",
    "- Probability: $p(x \\mid \\theta) = \\exp(\\theta x - A(\\theta))$  \n",
    "where $A(\\theta) = \\log(1 + e^\\theta)$.\n",
    "- The relationship between $\\theta$ and $\\mu$ is invertible so that\n",
    "  $$\n",
    "  \\mu = \\frac {1}{1 + exp(-\\theta)}\n",
    "  $$\n",
    "\n",
    "- **Remark**: The relationship between the original Bernoulli parameter $\\mu$ and\n",
    "the natural parameter $\\theta$ is known as the *sigmoid* or logistic function. The sigmoid function squeezes a real value into the range $(0, 1)$.\n",
    "\n",
    "Exponential families provide a convenient way to find conjugate pairs of distributions. Consider a random variable $X$ in the **exponential family**:\n",
    "\n",
    "$$\n",
    "p(x \\mid \\theta) = h(x) \\, \\exp \\big( \\langle \\theta, \\phi(x) \\rangle - A(\\theta) \\big).\n",
    "$$\n",
    "\n",
    "Every member of the exponential family has a **conjugate prior** of the form:\n",
    "\n",
    "$$\n",
    "p(\\theta \\mid \\gamma) = h_c(\\theta) \\, \\exp \\Big( \n",
    "\\langle \n",
    "\\begin{bmatrix} \\gamma_1 \\\\ \\gamma_2 \\end{bmatrix}, \n",
    "\\begin{bmatrix} \\theta \\\\ -A(\\theta) \\end{bmatrix} \n",
    "\\rangle - A_c(\\gamma) \n",
    "\\Big),\n",
    "$$\n",
    "where $\\gamma = \\begin{bmatrix} \\gamma_1 \\\\ \\gamma_2 \\end{bmatrix}$ has dimension $\\mathrm{dim}(\\theta)+1$, and the sufficient statistics of the conjugate prior are $\\begin{bmatrix} \\theta \\\\ -A(\\theta) \\end{bmatrix} $.\n",
    "\n",
    "\n",
    "As mentioned in the previous section, the main motivation for exponential families is that they have finite-dimensional sufficient statistics. Additionally, conjugate distributions are easy to write down, and the conjugate distributions also come from an exponential family."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954bd3c-8d37-41a4-a490-6ce497121def",
   "metadata": {},
   "source": [
    "## 6.7 Change of Variables/Inverse Transform\n",
    "We have a few set of distributions even though we have a lot of type of distributions\n",
    "$\\Longrightarrow$ how transformed random variables are distributed.\n",
    "**ex** : $X$, a random variable distributed according to the univariate normal distribution $\\mathcal{N}(0,,1)$, what is the distribution of $X^2$? or $\\frac{1}{2} (X_1 + X_2)$ for $X_1$ and $X_2$.\n",
    "\n",
    "One option to work out the distribution of $\\frac{1}{2} (X_1 + X_2)$ is to calculate the mean and variance of X1 and X2 and then combine them.\n",
    "\n",
    "**Notation remark** capital lette $X, Y$ : random variables, small letter $x, y$ : the values in the target space $\\mathcal{T}$ that the random variables take. pmfs of discrete random variables $X$ as $P(X = x)$. pdf is written as $f(x)$ for continous random variables $X$ and $F_X(x)$ is the cdf. \n",
    "\n",
    "Two approaches for obtaining distributions of transformations of random variables : \n",
    "- direct approach using the definition of a cumulative distribution function\n",
    "- change-of-variable(very widely used) approach that uses the chain rule of calculus (Section 5.2.2)\n",
    "\n",
    "$X$ with pmf $P(X = x)$, $U(x)$ an invertible function. The transformed random variable $ Y := U(X)$, with pmf $P(Y=y)$, then\n",
    "- *Transformation of interest (6.125a)*: $P(Y = y) = P(U(X)=y)$\n",
    "- *Inverse (6.125b* : $P(Y = y) = P(X = U^-1(y)) $\n",
    "\n",
    "Where we can observe that $x = U^-1 (y)$. Therefore, for discrete random variables, transformations directly change the individual events (with the probabilities appropriately transformed).\n",
    "\n",
    "Moment generating functions can also be used to study transformations of random variable see Casella, George, and Berger, Roger L. 2002. Statistical Inference. Duxbury.\n",
    "\n",
    "### 6.7.1 Distribution Function Technique\n",
    "uses cdf $F_X (x) = P (X \\leq x)$. Its differential is the pdf $f(x)$.\n",
    "$X$ Random variable, $U$ a function, the pdf of the random variable $Y := U(X)$ is found by:\n",
    "1. The **cumulative distribution function (CDF)**:\n",
    "\n",
    "$$\n",
    "F_Y(y) = P(Y \\leq y) \\tag{6.126}\n",
    "$$\n",
    "\n",
    "2. The **probability density function (PDF)** is obtained by differentiating the CDF:\n",
    "\n",
    "$$\n",
    "f(y) = \\frac{d}{dy} F_Y(y) \\tag{6.127}\n",
    "$$\n",
    "\n",
    "We also need to keep in mind that the domain of the random variable may  have changed due to the transformation by U\n",
    "#### Example 6.16 : (p216) \n",
    "we considered a strictly monotonically increasing function $f (x) = 3x^2$\n",
    "#### Theorem 6.15\n",
    "*Let $X$ be a continuous random variable with a strictly monotonic cumulative distribution function $F_X (x)$. Then the random variable $Y$ defined as*\n",
    "$$\n",
    "Y := F_X (X) \\tag{6.132}\n",
    "$$\n",
    "\n",
    "*has a uniform distribution.*\n",
    "\n",
    "Theorem 6.15 is known as the *probability integral transform*, and it is used to derive algorithms for sampling from distributions by transforming  the result of sampling from a uniform random variable (Bishop, 2006).\n",
    "\n",
    "### 6.7.2 Change of Variables\n",
    "The distribution function technique in Section 6.7.1 is derived from first principles, based on the definitions of cdfs and using properties of inverses, differentiation, and integration. This argument from first principles relies on two facts:\n",
    "1. We can transform the cdf of Y into an expression that is a cdf of X \n",
    "2. We can differentiate the cdf to obtain the pdf.\n",
    "\n",
    "For univariate functions, we use the **substitution rule of integration**:\n",
    "$$\n",
    "\\int f(g(x)) \\, g'(x) \\, dx = \\int f(u) \\, du, \n",
    "\\qquad \\text{where } u = g(x). \\tag{6.133}\n",
    "$$\n",
    "\n",
    "(based on the chain rule of calculus)\n",
    "#### Substitution Rule of Integration\n",
    "\n",
    "The rule is derived from the **chain rule** of calculus and the **fundamental theorem of calculus**.\n",
    "\n",
    "- **Fundamental theorem of calculus**:  \n",
    "  Differentiation and integration are (formal) inverses of each other.\n",
    "\n",
    "- **Differential intuition**:  \n",
    "  If $u = g(x)$, then  \n",
    "  $$\n",
    "  \\Delta u = g'(x) \\, \\Delta x \\;\\;\\;\\;\\; \\Rightarrow \\;\\;\\;\\;\\; du \\approx g'(x) \\, dx.\n",
    "  $$\n",
    "\n",
    "- **Substitution formula**:  \n",
    "  $$\n",
    "  \\int f(g(x)) \\, g'(x) \\, dx = \\int f(u) \\, du.\n",
    "  $$\n",
    "\n",
    "\n",
    "$X$ : random variable, $U$ : an *invertible* function, $ Y = U(X)$, $X$ has states $x \\in [a,b]$\n",
    "$$\n",
    "P(Y \\leq y) = P(U(X) \\leq ); \\tag{6.135}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P \\big( U(X) \\leq y \\big) \n",
    "= P \\big( U^{-1}(U(X)) \\leq U^{-1}(y) \\big) \n",
    "= P \\big( X \\leq U^{-1}(y) \\big).\n",
    "$$\n",
    "\n",
    "$$\n",
    "P \\big( X \\leq U^{-1}(y) \\big) \n",
    "= \\int_a^{U^{-1}(y)} f(x) \\, dx.\n",
    "\\tag{6.137}\n",
    "$$\n",
    "\n",
    "$$\n",
    "F_Y(y) = \\int_a^{U^{-1}(y)} f(x) \\, dx.\n",
    "\\tag{6.138}\n",
    "$$\n",
    "\n",
    "We obtain the pdf \n",
    "$$\n",
    "f(y) = \\frac{d}{dy} F_Y(y) \n",
    "= \\frac{d}{dy} \\int_a^{U^{-1}(y)} f(x)\\, dx \n",
    "\\tag{6.139}\n",
    "$$\n",
    "\n",
    "We are going to represent the formules with respect to $dy$\n",
    "$$\n",
    "\\int f\\!\\big(U^{-1}(y)\\big) \\, U^{-1'}(y)\\, dy \n",
    "= \\int f(x)\\, dx, \n",
    "\\qquad x = U^{-1}(y).\n",
    "\\tag{6.140}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(y) = f_xµ(U^{-1}(y)) \\cdot \\frac{d}{dy} U^{-1}(y). \\tag{6.142}\n",
    "$$\n",
    "Recall that we assumed that U is a strictly increasing function. For decreasing functions:\n",
    "$$\n",
    "f_Y(y) = f_X(U^{-1}(y)) \\cdot \\left| \\frac{d}{dy} U^{-1}(y) \\right| \\tag{6.143}\n",
    "$$\n",
    "\n",
    "*Change-of-variable technique* : The term $f_Y(y) = f_X(U^{-1}(y)) \\cdot \\left| \\frac{d}{dy} U^{-1}(y) \\right|$ in (6.143) measures how much a unit volume changes when applying $U$.\n",
    "\n",
    "**Remark** :Compared to the discrete case, the continuous case requires the additional factor $\\frac{d}{dy} U^{-1}(y)$ because $P(Y = y) = 0$ for all $y$. The **probability density function** is then   $f_Y(y) = f_X(U^{-1}(y)) \\cdot \\left| \\frac{d}{dy} U^{-1}(y) \\right|$  and cannot be interpreted as the probability of an event involving $y$.\n",
    "\n",
    "The determinant of the Jacobian matrix is used for multivariate random variablels.\n",
    "\n",
    "#### Theorem 6.16 (Multivariate change of variable)\n",
    "\n",
    "Let $f_X(x)$ be the probability density of a multivariate continuous random variable $X$. If the vector-valued function $y = U(x)$ is differentiable and invertible for all $x$ in its domain, then the density of $Y = U(X)$ is\n",
    "\n",
    "$$\n",
    "f(y) = f_x(U^{-1}(y)) \\cdot \\left| \\det \\big( \\frac{\\partial }{\\partial y} U^{-1}(y) \\big) \\right|.\n",
    "\\tag{6.144}\n",
    "$$\n",
    "**Exemple 6.17** :  While Example 6.17 is based on a bivariate random variable, which allows us to easily compute the matrix inverse, the preceding relation holds for higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabeb3a-92fa-450a-a3ce-d8dbf82e9893",
   "metadata": {},
   "source": [
    "### 6.8 Further Reading\n",
    "\n",
    "- **Introductory and self-study texts**: Grinstead and Snell (1997), Walpole et al. (2011).  \n",
    "- **Philosophical aspects of probability**: Hacking (2001).  \n",
    "- **Software-oriented approaches**: Downey (2014).  \n",
    "- **Exponential families**: Barndorff-Nielsen (2014).  \n",
    "- **Machine learning applications**:  \n",
    "  - Probabilistic modeling in ML tasks (Chapter 8).  \n",
    "  - Normalizing flows for transforming random variables (Jimenez Rezende and Mohamed, 2015).  \n",
    "  - Variational inference in neural networks (Goodfellow et al., 2016, Chapters 16–20).\n",
    "\n",
    "**Notes on continuous random variables**:  \n",
    "- Measure-theoretic issues are avoided for simplicity (Billingsley, 1995; Pollard, 2002).  \n",
    "- Conditional probabilities for continuous variables require care: $p(y \\mid x)$ where $X = x$ is a set of measure zero.  \n",
    "- More precise notation involves $\\mathbb{E}_y[f(y) \\mid \\sigma(x)]$.  \n",
    "\n",
    "**Advanced probability theory references**: Jaynes (2003), MacKay (2003), Jacod and Protter (2004), Grimmett and Welsh (2014), Shiryayev (1984), Lehmann and Casella (1998), Dudley (2002), Bickel and Doksum (2006), Çinlar (2011).  \n",
    "- Alternative approach: start from expectation and derive probability space properties (Whittle, 2000).  \n",
    "\n",
    "**Machine learning with probabilistic modeling**: MacKay (2003); Bishop (2006); Rasmussen and Williams (2006); Barber (2012); Murphy (2012).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81537d-d1b8-457c-8808-47c245036995",
   "metadata": {},
   "source": [
    "# Chapter 7 : Continuous Optimization\n",
    "Mathematical formulations are expressed as numerical optimization methods. The notion of “good” is determined by the objective function or the probabilistic model. Given an objective function, finding the best value is done using optimization algorithms.\n",
    "\n",
    "Two branches of optimization here: unconstrained and constrained optimization. We assume that  our objective function is differentiable Since we consider data and models in $Rù D$ , the optimization problems we face are continuous optimization problems, as opposed to combinatorial optimization  problems for discrete variables.\n",
    "\n",
    "\n",
    "**Global minimum** \n",
    "\n",
    "**Local minimum**\n",
    " \n",
    "Stationary points are the real roots of the derivative, that is, points that have zero gradient.\n",
    "For \n",
    "\n",
    "$$\n",
    "\\ell(x) = x^4 + 7x^3 + 5x^2 - 17x + 3\n",
    "$$\n",
    "\n",
    "we obtain the corresponding gradient as\n",
    "\n",
    "$$\n",
    "\\frac{d\\ell(x)}{dx} = 4x^3 + 21x^2 + 10x - 17\n",
    "$$\n",
    "\n",
    "To check whether a stationary point is a minimum or maximum, we need to take the derivative a second time and check whether the second derivative is positive or negative at the stationary\n",
    "point.\n",
    "\n",
    "We start from a point and follow the negative gradient since negative gradient indicates that we should go right.\n",
    "According to the Abel–Ruffini theorem, there is ingeneral no algebraic solution for polynomials of degree 5 or more.\n",
    "\n",
    "For convex functions all local minima are global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4bfa9a-0c7c-4eb4-997a-a34153e96c11",
   "metadata": {},
   "source": [
    "## 7.1 Optimization Using Gradient Descent\n",
    "On cherche à résoudre $ \\min_{x} f(x) $. Where $f : \\mathbb{R}^d \\to \\mathbb{R}$. is an objective function that captures the machine learning problem at hand.\n",
    "We assume that , $f$ is differentiable and no solution can be found analytically.\n",
    "\n",
    "*Gradient descent*: a first-order optimization algorithm to find local minimum of a function. one takes steps proportional to the negative of the gradient of the function at the current point. the gradient points in the direction of the steepest ascent. \n",
    "\n",
    "**Contour lines**:  set of lines where the function is at a certain value ($f (x) = c$ for some value $c \\in \\mathbb{R}$).\n",
    "\n",
    "The gradient points in a direction that is orthogonal to the contour lines of the function we wish to optimize.\n",
    "\n",
    "Gradient descent exploits the fact that $f(x_0)$ decreases fastest if one moves from $x_0$ in the direction of the negative gradient $- (\\nabla f)(x_0)^{\\top}$ of $f$ at $x_0$.\n",
    "\n",
    "If \n",
    "$$x_1 = x_0 - \\gamma ((\\nabla f)(x_0))^{\\top} \\tag{7.5}$$  \n",
    "for a small step-size $\\gamma \\geq 0$, then $f(x_1) \\leq f(x_0)$.\n",
    "\n",
    "The algorithm: If we want to find a local optimum $f(x^{\\ast})$ of a function  \n",
    "$$f : \\mathbb{R}^n \\to \\mathbb{R}, \\quad x \\mapsto f(x),$$  \n",
    "we start with an initial guess $x_0$ of the parameters we wish to optimize and then iterate according to  \n",
    "\n",
    "$$x_{i+1} = x_i - \\gamma_i (\\nabla f)(x_i)^{\\top} \\tag{7.6}$$  \n",
    "\n",
    "For suitable step-size $\\gamma_i$, the sequence  \n",
    "$$f(x_0) \\;\\geq\\; f(x_1) \\;\\geq\\; \\dots \\;\\; \\text{converges to a local minimum.}$$\n",
    "\n",
    "**Example 7.1** shows an example with a quadratic function in two dimensions\n",
    "\n",
    "**Remark** Gradient descent can be relatively slow close to the minimum: Its asymptotic rate of convergence is inferior to many other methods.\n",
    "### 7.1.1 Step-size\n",
    "The step-size is also called the learning rate. It is important because it can determine how accurate we are, too small $\\Longrightarrow$ slow, too large $\\Longrightarrow$ overshoot, fail to converge or diverge.\n",
    "\n",
    "**Adaptive gradient methods** : rescale the step-size at each iteration, depending on local properties of the function.\n",
    "\n",
    "*Two simple heuristics* : \n",
    "- When the function value increases after a gradient step, the step-size was too large. Undo the step and decrease the step-size.\n",
    "- When the function value decreases the step could have been larger. Try to increase the step-size.\n",
    "\n",
    "#### Example 7.2 (Solving a Linear Equation System) p230\n",
    "#### Remark\n",
    "The **condition number** is defined as  \n",
    "\n",
    "$$\n",
    "\\kappa = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}\n",
    "$$\n",
    "The ratio of the maximum to the minimum singular value of $A$.\n",
    "The condition number essentially measures the ratio of the most curved direction versus the least curved direction.\n",
    "\n",
    "Instead of directly solving $Ax = b$, one could instead solve  \n",
    "\n",
    "$$\n",
    "P^{-1}(Ax - b) = 0,\n",
    "$$  \n",
    "\n",
    "where $P$ is called the **preconditioner**.\n",
    "\n",
    "The goal is to design $P^{-1}$ such that $P^{-1} A$ has a better condition number,  \n",
    "but at the same time $P^{-1}$ is easy to compute.\n",
    "\n",
    "### 7.1.2 Gradient Descent With Momentum\n",
    "(to give gradient descent some memory| a “batch” optimization method)\n",
    "\n",
    "a method that introduces an additional term to remember what happened in the previous iteration.This memory dampens oscillations and smoothes out the gradient updates.\n",
    "\n",
    "The momentum-based method remembers the update $\\Delta x_i$ at each iteration $i$ and determines the next update as a linear combination of the current and previous gradients  \n",
    "\n",
    "$$x_{i+1} = x_i - \\gamma_i ((\\nabla f)(x_i))^\\top + \\alpha \\Delta x_i \\tag{7.11}$$  \n",
    "\n",
    "$$\\Delta x_i = x_i - x_{i-1} = \\alpha \\Delta x_{i-1} - \\gamma_{i-1} (\\nabla f(x_{i-1}))^\\top \\tag{7.12}$$  \n",
    "\n",
    "where $\\alpha \\in [0, 1]$.\n",
    "\n",
    "The momentum term is useful since it averages out different noisy estimates of the gradient.\n",
    "\n",
    "### 7.1.3 Stochastic Gradient Descent\n",
    "(a “cheap” approximation of the gradient)\n",
    "a stochastic approximation of the gradient descent method for minimizing an objective function that is written as a sum of differentiable functions\n",
    "\n",
    "**Stochastic**: we do not know the gradient precisely, but instead only know a noisy approximation to it.\n",
    "\n",
    "In machine learning, given $n = 1, \\ldots, N$ data points, we often consider objective functions that are the sum of the losses $L_n$ incurred by each example $n$.  \n",
    "In mathematical notation, we have the form  \n",
    "\n",
    "$$L(\\theta) = \\sum_{n=1}^{N} L_n(\\theta), \\tag{7.13}$$  \n",
    "\n",
    "where $\\theta$ is the vector of parameters of interest, i.e., we want to find $\\theta$ that minimizes $L$.\n",
    "\n",
    "An example at chapter 9 : \n",
    "$$L(\\theta) = - \\sum_{n=1}^{N} \\log p(y_n \\mid x_n, \\theta), \\tag{7.14}$$  \n",
    "\n",
    "where $x_n \\in \\mathbb{R}^D$ are the training inputs, $y_n$ are the training targets, and $\\theta$ are the parameters of the regression model.\n",
    "\n",
    "Optimization is performed using the full training set by updating the vector of parameters according to : \n",
    "\n",
    "$$\n",
    "\\theta_{i+1} = \\theta_i - \\gamma_i (\\nabla L(\\theta_i)) \n",
    "= \\theta_i - \\gamma_i \\sum_{n=1}^{N} (\\nabla L_n(\\theta_i))^{\\top}, \\tag{7.15}\n",
    "$$  \n",
    "\n",
    "for a suitable step-size parameter $\\gamma_i$.\n",
    "\n",
    "- **Batch Gradient Descent**: uses all $L_n$ ($n=1, \\dots, N$) to compute the gradient.  \n",
    "- **Mini-Batch Gradient Descent**: randomly selects a subset of $L_n$ to estimate the gradient.  \n",
    "- **Stochastic Gradient Descent (SGD)**: extreme case, uses only one randomly chosen $L_n$.  \n",
    "- **Key Insight**: convergence only requires the gradient estimate to be **unbiased**.  \n",
    "- The sum $\\sum_{n=1}^{N} \\nabla L_n(\\theta_i)$ is an **empirical estimate of the expected gradient**.  \n",
    "- Any **unbiased subsample** of the data can be used instead, reducing computation.  \n",
    "- **Reason for approximate gradients**: practical limits on CPU/GPU memory and computation time.  \n",
    "- **Large mini-batch**:\n",
    "  - More accurate gradient estimates (low variance).  \n",
    "  - Stable convergence.  \n",
    "  - Efficient with optimized matrix operations.  \n",
    "  - But more expensive per update.  \n",
    "- **Small mini-batch**:\n",
    "  - Faster updates.  \n",
    "  - Higher variance in gradient (adds noise).  \n",
    "  - Noise can help escape bad local minima.  \n",
    "- **Key trade-off**: accuracy vs. efficiency vs. ability to generalize.  \n",
    "- **Machine learning goal**: not exact minimization of training objective, but good **generalization performance**.  \n",
    "- **Widely used**: mini-batch SGD scales well to large problems (deep learning, topic models, reinforcement learning, Gaussian processes, etc.).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91f245-d841-4c73-96d6-ac1f353c478b",
   "metadata": {},
   "source": [
    "### 7.2 Constrained Optimization and Lagrange Multipliers\n",
    "We have additional contraints, that is, (primal problem)\n",
    "- **Problem type**: Constrained optimization.  \n",
    "- **Objective**: minimize a function $ f(x) $.  \n",
    "- **Constraints**:  \n",
    "  - Given by real-valued functions $ g_i: \\mathbb{R}^D \\to \\mathbb{R}$.  \n",
    "  - Condition: $ g_i(x) \\leq 0 $, for $ i = 1, \\dots, m$.  \n",
    "- **Interpretation**:  \n",
    "  - Feasible set = all $x $ that satisfy the constraints.  \n",
    "  - Optimization is restricted to this feasible set.  \n",
    "- **General form**:  \n",
    " $$\n",
    "  \\min_{x} f(x) \\quad \\text{subject to} \\quad g_i(x) \\leq 0 \\quad \\forall i = 1,\\dots,m \\tag{7.17}\n",
    "  $$\n",
    "\n",
    "  **indicator function** : Converting the contrained problem into an unconstrained one with \n",
    "  $$\n",
    "J(x) = f(x) + \\sum_{i=1}^{m} 1\\big(g_i(x)\\big), \\tag{7.18}\n",
    "$$\n",
    "\n",
    "where the indicator function $1(z)$ is defined as\n",
    "\n",
    "$$\n",
    "1(z) =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } z \\leq 0 \\\\\n",
    "\\infty & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\tag{7.19}\n",
    "$$\n",
    "\n",
    "This infinite step function is equally difficult to optimize. We can overcome this difficulty by introducing *Lagrange multipliers*. The idea of Lagrange multipliers is to replace the step function with a linear function.\n",
    "\n",
    "We associate to problem (7.17) the **Lagrangian** by introducing the Lagrange multipliers $\\lambda_i \\geq 0$ corresponding to each inequality constraint, so that\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x, \\lambda) = f(x) + \\sum_{i=1}^{m} \\lambda_i g_i(x), \\tag{7.20a}\n",
    "$$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x, \\lambda) = f(x) + \\lambda^\\top g(x), \\tag{7.20b}\n",
    "$$\n",
    "\n",
    "where in the last line we have concatenated all constraints $g_i(x)$ into a vector $g(x)$, and all the Lagrange multipliers into a vector $\\lambda \\in \\mathbb{R}^m$.\n",
    "\n",
    "*Lagrangian duality*: The idea of converting an optimization problem in one set of variables x (called the primal variables), into another optimization problem in a different set of variables λ (called the dual variables). The problem in (7.17) is known as the ***primal problem*** corresponding to the primal variables $x$. The associated **Lagrangian dual problem** to the primal problem is given by\n",
    "\n",
    "$$\n",
    "\\max_{\\lambda \\in \\mathbb{R}^m} \\mathcal{D}(\\lambda) \\quad \\text{subject to} \\quad \\lambda \\geq 0, \\tag{7.22}\n",
    "$$\n",
    "\n",
    "where $\\lambda$ are the dual variables and \n",
    "\n",
    "$$\n",
    "\\mathcal{D}(\\lambda) = \\min_{x \\in \\mathbb{R}^d} L(x, \\lambda).\n",
    "$$\n",
    "\n",
    "**Remark**: Two concept of interest: *minimax inequality* and *weak duality*\n",
    "p(234-235)\n",
    "- *minmax inequality* :\n",
    "  For any function with two arguments $\\varphi(x, y)$, the maximin is less than the minimax, i.e.,  \n",
    "\n",
    "$$\n",
    "\\max_{x} \\; \\min_{y} \\; \\varphi(x, y) \n",
    "\\;\\;\\; \\leq \\;\\;\\; \n",
    "\\min_{y} \\; \\max_{x} \\; \\varphi(x, y)\n",
    "\\tag{7.23}\n",
    "$$\n",
    "\n",
    "- *weak duality*:  uses (7.23) to show that primal values are always greater than or equal to dual values.\n",
    "\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^d} \\; \\max_{\\lambda \\geq 0} \\; \\mathcal{L}(x, \\lambda) \n",
    "\\;\\;\\; \\geq \\;\\;\\; \n",
    "\\max_{\\lambda \\geq 0} \\; \\min_{x \\in \\mathbb{R}^d} \\; \\mathcal{L}(x, \\lambda)\n",
    "\\tag{7.27}\n",
    "$$\n",
    "\n",
    "(see page 195 for more logic and reasoning)\n",
    "\n",
    "**Equality Contraints**:  We consider the following constrained optimization problem:  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{x} \\quad & f(x) \\\\\n",
    "\\text{subject to} \\quad & g_i(x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n",
    "& h_j(x) = 0, \\quad j = 1, \\ldots, n.\n",
    "\\end{aligned}\n",
    "\\tag{7.28}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc445991-6fe6-400f-a737-33aae6d3ad9d",
   "metadata": {},
   "source": [
    "## 7.3. Convex Optimization\n",
    "\n",
    "When f (·) is a convex function, and when the constraints involving g(·) and h(·) are convex sets.\n",
    "\n",
    "**strong duality** : The optimal solution of the dual problem is the same as the optimal solution of the primal problem.\n",
    "\n",
    "**Definition 7.2.** : A set $\\mathcal{C}$ is a **convex set** if for any $x, y \\in \\mathcal{C}$ and for any scalar $\\theta$ with $0 \\leq \\theta \\leq 1$, we have  \n",
    "\n",
    "$$\n",
    "\\theta x + (1 - \\theta)y \\in \\mathcal{C} . \\tag{7.29}\n",
    "$$\n",
    "\n",
    "**Definition 7.3.**   : Let function $f : \\mathbb{R}^D \\to \\mathbb{R}$ be a function whose domain is a convex set. The function $f$ is a **convex function** if for all $x, y$ in the domain of $f$, and for any scalar $\\theta$ with $0 \\leq \\theta \\leq 1$, we have  \n",
    "\n",
    "$$\n",
    "f\\big(\\theta x + (1 - \\theta)y\\big) \\leq \\theta f(x) + (1 - \\theta) f(y). \\tag{7.30}\n",
    "$$\n",
    "\n",
    "*Remark. A concave function is the negative of a convex function.*\n",
    "\n",
    "**Epigraph of the convex function**: The resulting filled-in set after *\"filling in\"* a convex function.\n",
    "\n",
    "**Convexity in terms of its gradient  $\\nabla_x f(x)$** : A function $f(x)$ is **convex** if and only if for any two points $x, y$ it holds that  \n",
    "\n",
    "$$\n",
    "f(y) \\;\\geq\\; f(x) + \\nabla_x f(x)^\\top (y - x).\n",
    "\\tag{7.31}\n",
    "$$\n",
    "\n",
    "If $f(x)$ is **twice differentiable**, then  \n",
    "\n",
    "$$\n",
    "(x) \\;\\text{is convex} \\;\\;\\Longleftrightarrow\\;\\; \\nabla_x^2 f(x) \\succeq 0,\n",
    "$$\n",
    "\n",
    "where $\\nabla_x^2 f(x)$ is the Hessian matrix of $f(x)$ and $\\succeq 0$ denotes **positive semidefiniteness**.\n",
    "\n",
    "**Remark**: The inequality in (7.30) is sometimes called *Jensen’s inequality*.\n",
    "\n",
    "In summary, a **constrained optimization problem** is called a **convex optimization problem** if  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{x} \\quad & f(x) \\\\\n",
    "\\text{subject to} \\quad & g_i(x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n",
    "& h_j(x) = 0, \\quad j = 1, \\ldots, n ,\n",
    "\\end{aligned}\n",
    "\\tag{7.38}\n",
    "$$\n",
    "\n",
    "where all functions $f(x)$ and $g_i(x)$ are convex functions, and all equality constraints $h_j(x) = 0$ define affine sets.\n",
    "\n",
    "### 7.3.1. Linear Programming\n",
    "**Linear Program** : All the preceding functions are linear, it has  $d$ variables and $m$ linear constraints.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{x \\in \\mathbb{R}^d} \\quad & c^\\top x \\\\\n",
    "\\text{subject to} \\quad & A x \\leq b, \\text{where}\\; A \\in \\mathbb{R}^{m \\times d}, \\; b \\in \\mathbb{R}^m .\n",
    "\\end{aligned}\n",
    "\\tag{7.39}\n",
    "$$\n",
    "\n",
    "\n",
    "The **Lagrangian** is given by  \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x, \\lambda) \\;=\\; c^\\top x \\;+\\; \\lambda^\\top (A x - b),\n",
    "\\tag{7.40}\n",
    "$$\n",
    "\n",
    "where $\\lambda \\in \\mathbb{R}^m$ is the vector of non-negative Lagrange multipliers.\n",
    "\n",
    "Rearranging the terms correponding to x yields: $$\n",
    "\\mathcal{L}(x, \\lambda) \\;=\\; (c + A^\\top \\lambda)^\\top x \\;-\\; \\lambda^\\top b .\n",
    "\\tag{7.41}\n",
    "$$\n",
    "\n",
    "Taking the derivative of $\\mathcal{L}(x, \\lambda)$ with respect to $x$ and setting it to zero gives  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}(x, \\lambda)}{\\partial x} = c + A^\\top \\lambda = 0.\n",
    "\\tag{7.42}\n",
    "$$\n",
    "\n",
    "**Dual Lagrangian and Dual Problem**\n",
    "\n",
    "From the Lagrangian, the **dual function** is  \n",
    "\n",
    "$$\n",
    "\\mathcal{D}(\\lambda) = - \\lambda^\\top b.\n",
    "$$\n",
    "\n",
    "We want to **maximize** $\\mathcal{D}(\\lambda)$ subject to the stationarity condition and non-negativity of the multipliers:  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{\\lambda \\in \\mathbb{R}^m} \\quad & - b^\\top \\lambda \\\\\n",
    "\\text{subject to} \\quad & c + A^\\top \\lambda = 0, \\\\\n",
    "& \\lambda \\geq 0 .\n",
    "\\end{aligned}\n",
    "\\tag{7.43}\n",
    "$$\n",
    "\n",
    "Notes:  \n",
    "- $d$ = number of primal variables, $m$ = number of primal constraints.  \n",
    "- The dual LP has $m$ variables, so depending on whether $m$ or $d$ is larger, we may choose to solve the **primal** (7.39) or **dual** (7.43) problem.\n",
    "\n",
    "### 7.3.2 Quadratic Programming\n",
    "**quadratic program**: the case of a convex quadratic objective function, where the constraints are affine. It has $d$ variables and $m$ linear constraints.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{x \\in \\mathbb{R}^d} \\quad & \\frac{1}{2} x^\\top Q x + c^\\top x \\\\\n",
    "\\text{subject to} \\quad & A x \\leq b, \\\\\n",
    "& A \\in \\mathbb{R}^{m \\times d}, \\; b \\in \\mathbb{R}^m, \\; c \\in \\mathbb{R}^d .\n",
    "\\end{aligned}\n",
    "\\tag{7.45}\n",
    "$$\n",
    "\n",
    "\n",
    "where the square symmetric matrix $Q \\in \\mathbb{R}^{d \\times d}$ is positive definite, and therefore the objective function is *convex*.\n",
    "\n",
    "The Lagrangian for the convex quadratic program is  \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x, \\lambda) = \\frac{1}{2} x^\\top Q x + c^\\top x + \\lambda^\\top (A x - b) \\tag{7.48a}\n",
    "$$\n",
    "\n",
    "which can also be rearranged as  \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x, \\lambda) = \\frac{1}{2} x^\\top Q x + (c + A^\\top \\lambda)^\\top x - \\lambda^\\top b.\n",
    "\\tag{7.48b}\n",
    "$$\n",
    "\n",
    "Taking the derivative with respect to $x$ and setting it to zero gives the stationarity condition:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}(x, \\lambda)}{\\partial x} = Q x + (c + A^\\top \\lambda) = 0 .\n",
    "\\tag{7.49}\n",
    "$$\n",
    "\n",
    "Since $Q$ is positive definite (and therefore invertible), we solve for $x$:\n",
    "\n",
    "$$\n",
    "x = - Q^{-1} (c + A^\\top \\lambda) .\n",
    "\\tag{7.50}\n",
    "$$\n",
    "\n",
    "Substituting this back into the Lagrangian gives the **dual function**:\n",
    "\n",
    "$$\n",
    "D(\\lambda) = -\\frac{1}{2} (c + A^\\top \\lambda)^\\top Q^{-1} (c + A^\\top \\lambda) - \\lambda^\\top b .\n",
    "\\tag{7.51}\n",
    "$$\n",
    "\n",
    "Finally, the **dual optimization problem** is  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{\\lambda \\in \\mathbb{R}^m} \\quad & -\\frac{1}{2} (c + A^\\top \\lambda)^\\top Q^{-1} (c + A^\\top \\lambda) - \\lambda^\\top b \\\\\n",
    "\\text{subject to} \\quad & \\lambda \\ge 0.\n",
    "\\end{aligned}\n",
    "\\tag{7.52}\n",
    "$$\n",
    "\n",
    "### 7.3.3 Legendre–Fenchel Transform and Convex Conjugate\n",
    "Convex set can be described by its *supporting hyperplanes*. A hyperplane is called a *supporting hyperplane* of a convex set if it intersects the convex set, and the convex set is contained on just one side of it.\n",
    "\n",
    "**The Legendre transform** : A concept that states : a convex functions can be equivalently described by a function of their gradient.\n",
    "\n",
    "**The Legendre-Fenchel transform (or the convex conjugate)** :  is a transformation (in the sense of a Fourier transform) from a convex differentiable function $f (x)$ to a function that depends on the tangents $s(x) = \\nabla_x f (x)$. (the transformation of $f(\\dot)$ and not the variable $x$ or the function evaluated at $x$.\n",
    "\n",
    "**Defintion 7.4.** : The **convex conjugate** of a function $f : \\mathbb{R}^D \\to \\mathbb{R}$ is a function $f^*$ defined by  \n",
    "\n",
    "$$\n",
    "f^*(s) = \\sup_{x \\in \\mathbb{R}^D} \\big( \\langle s, x \\rangle - f(x) \\big).\n",
    "\\tag{7.53}\n",
    "$$\n",
    "\n",
    "See p(243) in the book for more geometrical illutrations.\n",
    "\n",
    "The Legendre-Fenchel conjugate turns out to be quite useful for machine learning problems that can be expressed as convex optimization problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e26a950-04d0-4c10-b494-dd5b05a6ca4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
